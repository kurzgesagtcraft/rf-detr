变更记录
1.注释rfdetr\cli\main.py的trainer函数与Roboflow工作区相关的无用代码
2.rfdetr\config.py的TrainConfig类dataset_file改为"coco"
3.rfdetr\config.py的TrainConfig类dataset_dir改为"./dataset",pretrain_weights改为"./pretrain_weights/rf-detr-base-coco.pth"
4.创建dataset\train\_annotations.coco.json与dataset\train\example.json文件
5.rfdetr\main.py文件的get_args_parser函数的'--dataset_dir'设置default='./dataset',
'--pretrain_weights'default='./pretrain_weights/rf-detr-base-coco.pth',
'--encoder', default='vit_tiny'改为default='dinov2_windowed_registers_small'
dinov2预训练权重对源码模型结构适配
class Model:
    def __init__
169行添加for key in list(checkpoint['model'].keys()):
                if 'backbone.0.projector' in key:
                    del checkpoint['model'][key]

6.修复了 `if __name__ == '__main__'` 中直接调用不存在的 `main` 函数的错误，改为实例化 `Model` 并调用 `train` 方法。
7.在 `rfdetr/engine.py` 中添加了 `save_detection_images` 函数，用于在训练过程中将检测结果可视化并保存到 `./result` 目录。
#结果可视化函数新增导入
import os
from PIL import Image, ImageDraw
import torchvision.transforms.functional as F
from rfdetr.util import box_ops
import numpy as np
#新增的可视化函数
def denormalize_image(tensor, mean, std):
    """Denormalizes a tensor image with mean and standard deviation."""
    tensor = tensor.clone()
    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)
    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)
    tensor.mul_(std).add_(mean)
    return tensor

def save_detection_images(outputs, targets, samples, epoch, step, sub_step, output_dir="./result", threshold=0.5):
    """
    Saves images with predicted bounding boxes.
    """
    os.makedirs(output_dir, exist_ok=True)

    # Get predictions
    pred_logits = outputs['pred_logits'].softmax(-1)
    pred_boxes = outputs['pred_boxes']
    
    # Denormalize images for saving
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]

    for i in range(len(targets)):
        # Get image
        img_tensor = samples.tensors[i]
        img_tensor_denorm = denormalize_image(img_tensor, mean, std)
        img = F.to_pil_image(img_tensor_denorm.cpu())
        draw = ImageDraw.Draw(img)

        # Get original size and scale boxes
        orig_size = targets[i]['orig_size']
        img_w, img_h = orig_size.cpu().numpy()
        
        # Filter predictions by threshold
        scores, labels = pred_logits[i].max(-1)
        keep = scores > threshold
        
        boxes_to_draw = pred_boxes[i][keep]
        labels_to_draw = labels[keep]
        scores_to_draw = scores[keep]

        if boxes_to_draw.shape[0] == 0:
            continue

        # Convert boxes from [cx, cy, w, h] to [x1, y1, x2, y2] and scale
        scaled_boxes = box_ops.box_cxcywh_to_xyxy(boxes_to_draw)
        scaled_boxes = scaled_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_boxes.device)

        # Draw boxes
        for box, label, score in zip(scaled_boxes, labels_to_draw, scores_to_draw):
            box = box.cpu().tolist()
            draw.rectangle(box, outline="red", width=2)
            draw.text((box[0] + 2, box[1] + 2), f"L:{label.item()} S:{score.item():.2f}", fill="red")

        # Save image
        image_id = targets[i]['image_id'].item()
        save_path = os.path.join(output_dir, f"epoch{epoch}_step{step}_{sub_step}_img{image_id}.png")
        img.save(save_path)

8.将当前文件路径添加到python系统路径靠前防止导入其它项目的文件
9.在rfdetr\models\backbone\backbone.py文件Backbone类__init__方法中添加非窗口注意力强制下载预训练权重模型
添加if 'large' in name:
            num_layers = 24
        else:
            num_layers = 12
        
        processed_out_feature_indexes = out_feature_indexes
        if out_feature_indexes == [-1]:
            processed_out_feature_indexes = [num_layers - 1]
修改out_feature_indexes=out_feature_indexes,为out_feature_indexes=processed_out_feature_indexes,
10.rfdetr\models\backbone\dinov2_configs\dinov2_with_registers_base.json文件移除了"out_features": [
        "stage12"
    ],
    "out_indices": [
        12
    ],
    防止与定义的'--out_feature_indexes', default=[-1]改为default=[2, 5, 8, 11]冲突
11.创建rfdetr\models\backbone\dinov2_configs\dinov2_with_registers_small.json
12.rfdetr\models\backbone\dinov2.py文件74-77行
dino_config["out_features"] = [f"stage{i}" for i in out_feature_indexes]
改为processed_out_feature_indexes = out_feature_indexes
            if out_feature_indexes == [-1]:
                processed_out_feature_indexes = [11]
            dino_config["out_features"] = [f"stage{i+1}" for i in processed_out_feature_indexes]
13.rfdetr\datasets\coco.py文件def build(image_set, args, resolution):
    root = Path(args.coco_path)
    改为def build(image_set, args, resolution):
    root = Path(args.dataset_dir)
14.修改 rfdetr/datasets/transforms.py 中的 resize 函数
129行if target is None:
        return rescaled_image, None
改为# Calculate padding to make dimensions divisible by 14
    w, h = rescaled_image.size
    pad_w = (14 - (w % 14)) % 14
    pad_h = (14 - (h % 14)) % 14
    padded_image = F.pad(rescaled_image, (0, 0, pad_w, pad_h))

    if target is None:
        return padded_image, None

148行 h, w = size
    target["size"] = torch.tensor([h, w])

    if "masks" in target:
        target['masks'] = interpolate(
            target['masks'][:, None].float(), size, mode="nearest")[:, 0] > 0.5
    

    return rescaled_image, target
改为
# Update target size to padded image size
    target["size"] = torch.tensor([h + pad_h, w + pad_w])

    if "masks" in target:
        target['masks'] = interpolate(
            target['masks'][:, None].float(), (h + pad_h, w + pad_w), mode="nearest")[:, 0] > 0.5
    
    return padded_image, target

SquareResize 类__call__函数，
234行rescaled_img=F.resize(img, (size, size))
        w, h = rescaled_img.size
        if target is None:
            return rescaled_img, None
改为rescaled_img = F.resize(img, (size, size))

        # Calculate padding to make dimensions divisible by 14
        w, h = rescaled_img.size
        pad_w = (14 - (w % 14)) % 14
        pad_h = (14 - (h % 14)) % 14
        padded_image = F.pad(rescaled_img, (0, 0, pad_w, pad_h))

        if target is None:
            return padded_image, None

254行target["size"] = torch.tensor([h, w])

        return rescaled_img, target
改为# Update target size to padded image size
        target["size"] = torch.tensor([h + pad_h, w + pad_w])

        return padded_image, target
以确保图像尺寸能够被 14 整除。
15.rfdetr\models\backbone\dinov2_with_windowed_attn.py文件WindowedDinov2WithRegistersEmbeddings类forward函数
291行插入# Ensure height and width are divisible by patch_size * num_windows
        adjusted_height = (height // (self.config.patch_size * self.config.num_windows)) * (self.config.patch_size * self.config.num_windows)
        adjusted_width = (width // (self.config.patch_size * self.config.num_windows)) * (self.config.patch_size * self.config.num_windows)

        # Resize pixel_values to adjusted_height, adjusted_width before patch embedding
        if adjusted_height != height or adjusted_width != width:
            pixel_values = F.resize(pixel_values, (adjusted_height, adjusted_width))
            batch_size, _, height, width = pixel_values.shape # Update height and width

315行 windowed_pixel_tokens = pixel_tokens_with_pos_embed.view(batch_size, num_windows, num_h_patches_per_window, num_windows, num_h_patches_per_window, -1)
改为windowed_pixel_tokens = pixel_tokens_with_pos_embed.view(batch_size, num_windows, num_h_patches_per_window, num_windows, num_w_patches_per_window, -1)

1101行batch_size, _, height, width = pixel_values.shape
                    patch_size = self.config.patch_size
改为
# Get the actual height and width from the hidden_state (after removing CLS and register tokens)
                    # hidden_state shape: (batch_size, num_patches, hidden_size)
                    batch_size, num_patches, hidden_size = hidden_state.shape
                    patch_size = self.config.patch_size
                    
                    # Calculate height and width based on num_patches and patch_size
                    # Assuming square patches and original image was square or close to square
                    # This might need adjustment if aspect ratio is highly variable
                    height = int(math.sqrt(num_patches)) * patch_size
                    width = int(math.sqrt(num_patches)) * patch_size

16.rfdetr\models\backbone\dinov2_with_windowed_attn.py文件WindowedDinov2WithRegistersBackbone类forward函数
1119行num_h_patches_per_window = num_h_patches // self.config.num_windows
                        num_w_patches_per_window = num_w_patches // self.config.num_windows
                        hidden_state = hidden_state.reshape(B // num_windows_squared, num_windows_squared * HW, C)
                        hidden_state = hidden_state.view(B // num_windows_squared, self.config.num_windows, self.config.num_windows, num_h_patches_per_window, num_w_patches_per_window, C)
                        hidden_state = hidden_state.permute(0, 1, 3, 2, 4, 5)

                    hidden_state = hidden_state.reshape(batch_size, num_h_patches, num_w_patches, -1)
改为# Calculate num_h_patches_per_window and num_w_patches_per_window from HW
                        # HW is the number of patches per window (including CLS token if present)
                        # Assuming square patches within a window
                        # num_patches_per_window = HW # This line is problematic, HW is already the number of patches per window
                        
                        # Recalculate num_h_patches_per_window and num_w_patches_per_window based on total patches and number of windows
                        num_h_patches_per_window = num_h_patches // self.config.num_windows
                        num_w_patches_per_window = num_w_patches // self.config.num_windows

                        hidden_state = hidden_state.reshape(B // num_windows_squared, num_windows_squared, num_h_patches_per_window, num_w_patches_per_window, C)
                        hidden_state = hidden_state.permute(0, 1, 3, 2, 4)
                        hidden_state = hidden_state.reshape(batch_size, num_h_patches, num_w_patches, C)
                    else:
                        hidden_state = hidden_state.reshape(batch_size, num_h_patches, num_w_patches, -1)

然后
1105行# this was actually a bug in the original implementation that we copied here,
                    # cause normally the order is height, width
                    # Get the actual height and width from the hidden_state (after removing CLS and register tokens)
                    # hidden_state shape: (batch_size, num_patches, hidden_size)
                    batch_size, num_patches, hidden_size = hidden_state.shape
                    patch_size = self.config.patch_size
                    
                    # Calculate height and width based on num_patches and patch_size
                    # Assuming square patches and original image was square or close to square
                    # This might need adjustment if aspect ratio is highly variable
                    height = int(math.sqrt(num_patches)) * patch_size
                    width = int(math.sqrt(num_patches)) * patch_size

                    num_h_patches = height // patch_size
                    num_w_patches = width // patch_size
改为
batch_size = pixel_values.shape[0]
                    height, width = pixel_values.shape[2:]
                    patch_size = self.config.patch_size
                    
                    adjusted_height = (height // (self.config.patch_size * self.config.num_windows)) * (self.config.patch_size * self.config.num_windows)
                    adjusted_width = (width // (self.config.patch_size * self.config.num_windows)) * (self.config.patch_size * self.config.num_windows)

                    num_h_patches = adjusted_height // patch_size
                    num_w_patches = adjusted_width // patch_size

17.rfdetr\models\backbone\dinov2_with_windowed_attn.py文件WindowedDinov2WithRegistersLayer类forward函数
628行hidden_states = hidden_states.view(B // num_windows_squared, num_windows_squared * HW, C)

        self_attention_outputs = self.attention(
            self.norm1(hidden_states),  # in Dinov2WithRegisters, layernorm is applied before self-attention
            head_mask,
            output_attentions=output_attentions,
        )
        attention_output = self_attention_outputs[0]

        if run_full_attention:
            # reshape x to add windows back
            B, HW, C = hidden_states.shape
            num_windows_squared = self.num_windows ** 2
            # hidden_states = hidden_states.view(B * num_windows_squared, HW // num_windows_squared, C)
            attention_output = attention_output.view(B * num_windows_squared, HW // num_windows_squared, C)
改为
# Calculate the number of patches per window (excluding CLS and register tokens)
            # HW is 1 (CLS) + num_register_tokens + num_patches_per_window
            num_patches_per_window_actual = HW - 1 - self.config.num_register_tokens
            
            # Calculate the total number of tokens (including CLS and register tokens)
            total_tokens = (1 + self.config.num_register_tokens + num_patches_per_window_actual) * num_windows_squared
            
            # Reshape to original batch size and total tokens (including CLS and register tokens)
            hidden_states = hidden_states.view(B // num_windows_squared, total_tokens, C)

        self_attention_outputs = self.attention(
            self.norm1(hidden_states),  # in Dinov2WithRegisters, layernorm is applied before self-attention
            head_mask,
            output_attentions=output_attentions,
        )
        attention_output = self_attention_outputs[0]

        if run_full_attention:
            attention_output = attention_output.view(shortcut.shape)

18.在rfdetr\models\backbone\dinov2_with_windowed_attn.py文件 WindowedDinov2WithRegistersLayer类 的 __init__ 方法中添加 self.config = config。

19.rfdetr\main.py文件
添加from torch.utils.tensorboard import SummaryWriter
class Model:
def train(self, callbacks: DefaultDict[str, List[Callable]], **kwargs):
改为
def train(self, callbacks: DefaultDict[str, List[Callable]], writer=None, **kwargs):
624行effective_batch_size, args.clip_max_norm, ema_m=self.ema_m, schedules=schedules, 
                num_training_steps_per_epoch=num_training_steps_per_epoch,
                vit_encoder_num_layers=args.vit_encoder_num_layers, args=args, callbacks=callbacks)
改为
effective_batch_size, args.clip_max_norm, ema_m=self.ema_m, schedules=schedules,
                num_training_steps_per_epoch=num_training_steps_per_epoch,
                vit_encoder_num_layers=args.vit_encoder_num_layers, args=args, callbacks=callbacks, writer=writer)
677行self.ema_m.module, criterion, postprocessors, data_loader_val, base_ds, device, args=args
改为self.ema_m.module, criterion, postprocessors, data_loader_val, base_ds, device, args=args, writer=writer, epoch=epoch

1260行model = Model(**config)
        model.train(callbacks=DefaultDict(list), **config)
改为writer = SummaryWriter('runs')
        model = Model(**config)
        model.train(callbacks=DefaultDict(list), writer=writer, **config)
        writer.close()

20.rfdetr\engine.py文件
添加import time

train_one_epoch方法添加writer=None,参数
211行添加
# For FPS calculation
forward_pass_times = []
223行
            outputs = model(samples)
改为
            start_time = time.time()
            outputs = model(samples)
            end_time = time.time()
            # We only measure forward pass time for batch size 1 for a realistic FPS measurement
            if samples.tensors.shape[0] == 1:
                forward_pass_times.append(end_time - start_time)

284行
stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
    if writer:
        for k, v in stats.items():
            if isinstance(v, (int, float)):
                writer.add_scalar(f'eval/{k}', v, epoch)
    if coco_evaluator is not None:
        if "bbox" in postprocessors.keys():
            stats["coco_eval_bbox"] = coco_evaluator.coco_eval["bbox"].stats.tolist()
            if writer:
                for i, stat in enumerate(coco_evaluator.coco_eval["bbox"].stats):
                    writer.add_scalar(f'eval/bbox_stat_{i}', stat, epoch)
改为
# Calculate FPS
    if forward_pass_times:
        avg_forward_time = np.mean(forward_pass_times)
        fps = 1 / avg_forward_time
    else:
        fps = 0
    
    print(f"FPS (batch_size=1): {fps:.2f}")

    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
    stats['fps'] = fps

    if coco_evaluator is not None:
        if "bbox" in postprocessors.keys():
            stats["coco_eval_bbox"] = coco_evaluator.coco_eval["bbox"].stats.tolist()
            map_50_95 = coco_evaluator.coco_eval['bbox'].stats[0]
            map_50 = coco_evaluator.coco_eval['bbox'].stats[1]
            stats['mAP@.50-.95'] = map_50_95
            stats['mAP@.50'] = map_50
            print(f"mAP@.50-.95: {map_50_95:.4f}")
            print(f"mAP@.50: {map_50:.4f}")

    if writer:
        for k, v in stats.items():
            if isinstance(v, (int, float)):
                writer.add_scalar(f'eval/{k}', v, epoch)
        if 'mAP@.50-.95' in stats:
            writer.add_scalar('eval/mAP_50-95', stats['mAP@.50-.95'], epoch)
            writer.add_scalar('eval/mAP_50', stats['mAP@.50'], epoch)
            writer.add_scalar('eval/FPS', stats['fps'], epoch)



180行添加if writer:
            writer.add_scalar('train/loss', loss_value, it)
            writer.add_scalar('train/class_error', loss_dict_reduced["class_error"], it)
            writer.add_scalar('train/lr', optimizer.param_groups[0]["lr"], it)
191行def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, args=None):
改为def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, args=None):

271行添加if writer:
        for k, v in stats.items():
            if isinstance(v, (int, float)):
                writer.add_scalar(f'eval/{k}', v, epoch)
278行添加if writer:
                for i, stat in enumerate(coco_evaluator.coco_eval["bbox"].stats):
                    writer.add_scalar(f'eval/bbox_stat_{i}', stat, epoch)

21.rfdetr\main.py的train方法中
535行添加# Select 100 representative images for visualization
        image_ids = list(dataset_train.coco.imgs.keys())
        if len(image_ids) > 100:
            # Use a fixed seed for reproducibility of selected images
            random.seed(args.seed)
            representative_image_ids = random.sample(image_ids, 100)
        else:
            representative_image_ids = image_ids
        # convert to a set for faster lookup
        representative_image_ids = set(representative_image_ids)
691行
 vit_encoder_num_layers=args.vit_encoder_num_layers, args=args, callbacks=callbacks, writer=writer)
 改为
 vit_encoder_num_layers=args.vit_encoder_num_layers, args=args, callbacks=callbacks, writer=writer,
                representative_image_ids=representative_image_ids,
                coco_dataset=dataset_train.coco)
22.rfdetr\engine.py文件train_one_epoch方法添加representative_image_ids=None,coco_dataset=None,参数
133行save_detection_images(outputs, new_targets, new_samples, epoch, data_iter_step, i)
改为image_id = new_targets[0]['image_id'].item()
    if representative_image_ids and image_id in representative_image_ids:
        save_detection_images(outputs, new_targets, new_samples, epoch, data_iter_step, i, coco_dataset=coco_dataset)

最后函数改为
def save_detection_images(outputs, targets, samples, epoch, step, sub_step, output_dir="./result", threshold=0.5, coco_dataset=None):
    """
    Saves images with predicted and ground truth bounding boxes.
    """
    # Get predictions
    pred_logits = outputs['pred_logits'].softmax(-1)
    pred_boxes = outputs['pred_boxes']
    
    # Denormalize images for saving
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]

    for i in range(len(targets)):
        image_id = targets[i]['image_id'].item()
        
        # Get original image filename
        img_info = coco_dataset.loadImgs(image_id)[0]
        img_filename = img_info['file_name']
        img_name_without_ext = os.path.splitext(img_filename)[0]

        # Create a directory for the image if it doesn't exist
        img_output_dir = os.path.join(output_dir, img_name_without_ext)
        os.makedirs(img_output_dir, exist_ok=True)

        # Get image
        img_tensor = samples.tensors[i]
        img_tensor_denorm = denormalize_image(img_tensor, mean, std)
        img = F.to_pil_image(img_tensor_denorm.cpu())
        draw = ImageDraw.Draw(img)

        # Get original size and scale boxes
        orig_size = targets[i]['orig_size']
        img_h, img_w = orig_size.cpu().numpy()
        
        # --- Draw Ground Truth Boxes (Green) ---
        gt_boxes = targets[i]['boxes']
        gt_labels = targets[i]['labels']
        
        # Convert GT boxes from [cx, cy, w, h] to [x1, y1, x2, y2] and scale
        scaled_gt_boxes = box_ops.box_cxcywh_to_xyxy(gt_boxes)
        scaled_gt_boxes = scaled_gt_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_gt_boxes.device)

        for box, label in zip(scaled_gt_boxes, gt_labels):
            box = box.cpu().tolist()
            draw.rectangle(box, outline="lime", width=2)
            draw.text((box[0] + 2, box[1] - 12), f"GT_L:{label.item()}", fill="lime")

        # --- Draw Predicted Boxes (Red) ---
        scores, labels = pred_logits[i].max(-1)
        keep = scores > threshold
        
        boxes_to_draw = pred_boxes[i][keep]
        labels_to_draw = labels[keep]
        scores_to_draw = scores[keep]

        if boxes_to_draw.shape[0] > 0:
            # Convert boxes from [cx, cy, w, h] to [x1, y1, x2, y2] and scale
            scaled_boxes = box_ops.box_cxcywh_to_xyxy(boxes_to_draw)
            scaled_boxes = scaled_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_boxes.device)

            # Draw boxes
            for box, label, score in zip(scaled_boxes, labels_to_draw, scores_to_draw):
                box = box.cpu().tolist()
                draw.rectangle(box, outline="red", width=2)
                draw.text((box[0] + 2, box[1] + 2), f"L:{label.item()} S:{score.item():.2f}", fill="red")

        # Save image
        save_path = os.path.join(img_output_dir, f"epoch{epoch}_step{step}.png")
        img.save(save_path)

23.rfdetr\engine.py文件train_one_epoch方法
132行
if epoch % 5 == 0 and data_iter_step % 10 == 0:
                    try:
                        image_id = new_targets[0]['image_id'].item()
                        if representative_image_ids and image_id in representative_image_ids:
                            save_detection_images(outputs, new_targets, new_samples, epoch, data_iter_step, i, coco_dataset=coco_dataset)
改为
if epoch % 5 == 0:
                    try:
                        # Iterate over each target in the sub-batch
                        for j in range(len(new_targets)):
                            image_id = new_targets[j]['image_id'].item()
                            if representative_image_ids and image_id in representative_image_ids:
                                # Pass only the relevant data for the specific image
                                single_output = {k: v[j:j+1] for k, v in outputs.items()}
                                single_target = [new_targets[j]]
                                single_sample = NestedTensor(new_samples.tensors[j:j+1], new_samples.mask[j:j+1])
                                save_detection_images(single_output, single_target, single_sample, epoch, data_iter_step, i, coco_dataset=coco_dataset)
def evaluate
if writer:
        for k, v in stats.items():
            if isinstance(v, (int, float)):
                writer.add_scalar(f'eval/{k}', v, epoch)
        if 'mAP@.50-.95' in stats:
            writer.add_scalar('eval/mAP_50-95', stats['mAP@.50-.95'], epoch)
            writer.add_scalar('eval/mAP_50', stats['mAP@.50'], epoch)
            writer.add_scalar('eval/FPS', stats['fps'], epoch)
        if "segm" in postprocessors.keys():
            stats["coco_eval_masks"] = coco_evaluator.coco_eval["segm"].stats.tolist()
改为
if writer:
        # Log main stats
        for k, v in stats.items():
            if isinstance(v, (int, float)) and k not in ['fps', 'mAP@.50-.95', 'mAP@.50']:
                writer.add_scalar(f'eval/{k}', v, epoch)
        
        # Log specific metrics for clarity in TensorBoard
        if 'mAP@.50-.95' in stats:
            writer.add_scalar('eval/mAP@.50-.95', stats['mAP@.50-.95'], epoch)
        if 'mAP@.50' in stats:
            writer.add_scalar('eval/mAP@.50', stats['mAP@.50'], epoch)
        if 'fps' in stats:
            writer.add_scalar('eval/FPS', stats['fps'], epoch)

    if coco_evaluator and "segm" in postprocessors.keys():
        stats["coco_eval_masks"] = coco_evaluator.coco_eval["segm"].stats.tolist()
309行
if len(stats["coco_eval_bbox"]) > 1: # Ensure stats has enough elements
                map_50_95 = 0.0
                map_50 = 0.0
                if len(stats["coco_eval_bbox"]) > 1:
                    map_50_95 = stats["coco_eval_bbox"][0]
                    map_50 = stats["coco_eval_bbox"][1]
                stats['mAP@.50-.95'] = map_50_95
                stats['mAP@.50'] = map_50
                # Add mAP values to metric_logger
                metric_logger.add_meter("mAP@.50-.95", utils.SmoothedValue(window_size=1, fmt="{value:.4f}"))
                metric_logger.add_meter("mAP@.50", utils.SmoothedValue(window_size=1, fmt="{value:.4f}"))
                metric_logger.meters["mAP@.50-.95"].update(map_50_95)
                metric_logger.meters["mAP@.50"].update(map_50)
                print(f"mAP@.50-.95: {map_50_95:.4f}")
                print(f"mAP@.50: {map_50:.4f}")

    if writer and epoch is not None:
        writer.add_scalar('eval/FPS', fps, epoch)
        # Log all stats from metric_logger
        for k, meter in metric_logger.meters.items():
            if k != 'coco_eval_bbox': # Exclude coco_eval_bbox as it's a list
                writer.add_scalar(f'eval/{k}', meter.global_avg, epoch)
改为
stats['mAP@.50-.95'] = stats["coco_eval_bbox"][0]
            stats['mAP@.50'] = stats["coco_eval_bbox"][1]

    if writer and epoch is not None:
        # Log main stats
        for k, v in stats.items():
            if isinstance(v, (int, float)) and k not in ['fps', 'mAP@.50-.95', 'mAP@.50', 'coco_eval_bbox']:
                writer.add_scalar(f'eval/{k}', v, epoch)
        
        # Log specific metrics for clarity in TensorBoard
        if 'mAP@.50-.95' in stats:
            writer.add_scalar('eval/mAP@.50-.95', stats['mAP@.50-.95'], epoch)
        if 'mAP@.50' in stats:
            writer.add_scalar('eval/mAP@.50', stats['mAP@.50'], epoch)
        if 'fps' in stats:
            writer.add_scalar('eval/FPS', stats['fps'], epoch)

def save_detection_images
# Save image
        save_path = os.path.join(img_output_dir, f"epoch{epoch}_step{step}.png")
改为
# Save image with a unique name
        save_path = os.path.join(img_output_dir, f"epoch{epoch}_step{step}_sub{sub_step}.png")

        
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


v4改动
24.rfdetr\engine.py文件
def train_one_epoch
83行插入
saved_representative_images_this_epoch = set()

131行
# Save images periodically to check detection results
                if epoch % 5 == 0:
                    try:
                        # Iterate over each target in the sub-batch
                        for j in range(len(new_targets)):
                            image_id = new_targets[j]['image_id'].item()
                            if representative_image_ids and image_id in representative_image_ids:
                                # Pass only the relevant data for the specific image
                                single_output = {k: v[j:j+1] for k, v in outputs.items()}
                                single_target = [new_targets[j]]
                                single_sample = NestedTensor(new_samples.tensors[j:j+1], new_samples.mask[j:j+1])
                                save_detection_images(single_output, single_target, single_sample, epoch, data_iter_step, i, coco_dataset=coco_dataset)
                    except Exception as e:
                        print(f"Error saving detection image: {e}")
改为
# Save images periodically to check detection results
                if representative_image_ids:
                    for j in range(len(new_targets)):
                        image_id = new_targets[j]['image_id'].item()
                        # print(f"Checking image_id: {image_id}, is in representative: {image_id in representative_image_ids}, is already saved: {image_id in saved_representative_images_this_epoch}")
                        if image_id in representative_image_ids and image_id not in saved_representative_images_this_epoch:
                            try:
                                print(f"Saving detection image for image_id: {image_id} in epoch {epoch}")
                                single_output = {k: v[j:j+1] for k, v in outputs.items()}
                                single_target = [new_targets[j]]
                                single_sample = NestedTensor(new_samples.tensors[j:j+1], new_samples.mask[j:j+1])
                                save_detection_images(single_output, single_target, single_sample, epoch, coco_dataset=coco_dataset, criterion=criterion)
                                saved_representative_images_this_epoch.add(image_id)
                            except Exception as e:
                                print(f"Error saving detection image {image_id} in epoch {epoch}: {e}")


def evaluate
map_50_95 = coco_evaluator.coco_eval['bbox'].stats[0]
            map_50 = coco_evaluator.coco_eval['bbox'].stats[1]
            stats['mAP@.50-.95'] = map_50_95
            stats['mAP@.50'] = map_50
            print(f"mAP@.50-.95: {map_50_95:.4f}")
            print(f"mAP@.50: {map_50:.4f}")

    if writer:
        # Log main stats
        for k, v in stats.items():
            if isinstance(v, (int, float)) and k not in ['fps', 'mAP@.50-.95', 'mAP@.50']:
改为
# Ensure mAP stats are calculated and available before writing
            if stats["coco_eval_bbox"]:
                map_50_95 = stats["coco_eval_bbox"][0]
                map_50 = stats["coco_eval_bbox"][1]
                stats['mAP@.50-.95'] = map_50_95
                stats['mAP@.50'] = map_50
                print(f"mAP@.50-.95: {map_50_95:.4f}")
                print(f"mAP@.50: {map_50:.4f}")

    if writer and epoch is not None:
        # Log main stats
        for k, v in stats.items():
            if isinstance(v, (int, float)) and k not in ['fps', 'mAP@.50-.95', 'mAP@.50', 'coco_eval_bbox']:
之后   
# Ensure mAP stats are calculated and available before writing
            if stats["coco_eval_bbox"]:
                map_50_95 = stats["coco_eval_bbox"][0]
                map_50 = stats["coco_eval_bbox"][1]
                stats['mAP@.50-.95'] = map_50_95
                stats['mAP@.50'] = map_50
                print(f"mAP@.50-.95: {map_50_95:.4f}")
                print(f"mAP@.50: {map_50:.4f}")

    if writer and epoch is not None:
        # Log main stats
        for k, v in stats.items():
            if isinstance(v, (int, float)) and k not in ['fps', 'mAP@.50-.95', 'mAP@.50', 'coco_eval_bbox']:
                writer.add_scalar(f'eval/{k}', v, epoch)
        
        # Log specific metrics for clarity in TensorBoard
        if 'mAP@.50-.95' in stats:
            writer.add_scalar('eval/mAP@.50-.95', stats['mAP@.50-.95'], epoch)
        if 'mAP@.50' in stats:
            writer.add_scalar('eval/mAP@.50', stats['mAP@.50'], epoch)
        if 'fps' in stats:
            writer.add_scalar('eval/FPS', stats['fps'], epoch)
改为
if len(stats["coco_eval_bbox"]) > 1: # Ensure stats has enough elements
                map_50_95 = 0.0
                map_50 = 0.0
                if len(stats["coco_eval_bbox"]) > 1:
                    map_50_95 = stats["coco_eval_bbox"][0]
                    map_50 = stats["coco_eval_bbox"][1]
                stats['mAP@.50-.95'] = map_50_95
                stats['mAP@.50'] = map_50
                # Add mAP values to metric_logger
                metric_logger.add_meter("mAP@.50-.95", utils.SmoothedValue(window_size=1, fmt="{value:.4f}"))
                metric_logger.add_meter("mAP@.50", utils.SmoothedValue(window_size=1, fmt="{value:.4f}"))
                metric_logger.meters["mAP@.50-.95"].update(map_50_95)
                metric_logger.meters["mAP@.50"].update(map_50)
                print(f"mAP@.50-.95: {map_50_95:.4f}")
                print(f"mAP@.50: {map_50:.4f}")

    if writer and epoch is not None:
        writer.add_scalar('eval/FPS', fps, epoch)
        # Log all stats from metric_logger
        for k, meter in metric_logger.meters.items():
            if k != 'coco_eval_bbox': # Exclude coco_eval_bbox as it's a list
                writer.add_scalar(f'eval/{k}', meter.global_avg, epoch)


                 

def save_detection_images(outputs, targets, samples, epoch, step, sub_step, output_dir="./result", threshold=0.7, coco_dataset=None):
改为
def save_detection_images(outputs, targets, samples, epoch, output_dir="./result", coco_dataset=None, criterion=None):
    """
    Saves images by drawing matched predicted boxes on top of the original images
    which already contain ground truth boxes.
    """
    # Get image_id and filename
    target = targets[0]
    image_id = target['image_id'].item()
    img_info = coco_dataset.loadImgs(image_id)[0]
    img_filename = img_info['file_name']

    # Load the original image from the visual dataset (which has GT boxes)
    original_image_path = os.path.join("dataset/visual/train", img_filename)
    if not os.path.exists(original_image_path):
        print(f"Warning: Original image not found at {original_image_path}, skipping.")
        return
    
    img = Image.open(original_image_path).convert('RGB')
    draw = ImageDraw.Draw(img)

    # Get original size for scaling boxes
    orig_size = target['orig_size']
    img_h, img_w = orig_size.cpu().numpy()

    # Prepare output directory
    img_name_without_ext = os.path.splitext(img_filename)[0]
    img_output_dir = os.path.join(output_dir, img_name_without_ext)
    os.makedirs(img_output_dir, exist_ok=True)
    print(f"Saving image to directory: {img_output_dir}")

    if criterion:
        # Use matcher to find corresponding predicted boxes
        indices = criterion.matcher(outputs, targets)
        pred_idx, _ = indices[0]  # We only need the indices of the predicted boxes

        # --- Draw Matched Predicted Boxes (Red) ---
        pred_boxes = outputs['pred_boxes'][0][pred_idx]
        pred_logits = outputs['pred_logits'][0][pred_idx]
        scores, labels = pred_logits.softmax(-1).max(-1)

        if pred_boxes.shape[0] > 0:
            scaled_pred_boxes = box_ops.box_cxcywh_to_xyxy(pred_boxes)
            scaled_pred_boxes = scaled_pred_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_pred_boxes.device)
            
            for box, label, score in zip(scaled_pred_boxes, labels, scores):
                box = box.cpu().tolist()
                category_id = label.item()
                category_name = coco_dataset.cats[category_id]['name'] if category_id in coco_dataset.cats else f"ID:{category_id}"
                
                draw.rectangle(box, outline="red", width=2)
                
                text = f"{category_name} {score.item():.2f}"
                text_position = (box[0] + 2, box[1] + 2)
                
                try:
                    text_bbox = draw.textbbox(text_position, text)
                    draw.rectangle(text_bbox, fill="red")
                    draw.text(text_position, text, fill='white')
                except AttributeError:  # Fallback for older Pillow versions
                    text_size = draw.textsize(text)
                    draw.rectangle([text_position[0], text_position[1], text_position[0] + text_size[0], text_position[1] + text_size[1]], fill="red")
                    draw.text(text_position, text, fill='white')
    else:
        print("Warning: `criterion` not provided to `save_detection_images`. Cannot determine which boxes are used for loss. Skipping image save.")
        return

    # Save the modified image
    save_path = os.path.join(img_output_dir, f"epoch_{epoch}.png")
    img.save(save_path)




23.rfdetr\main.py文件class Model:def train
添加print(f"Selected {len(representative_image_ids)} representative image IDs for visualization.")
1165行
if args.use_ema:
                ema_test_stats, _ = evaluate(
                    self.ema_m.module, criterion, postprocessors, data_loader_val, base_ds, device, args=args, writer=writer, epoch=epoch
                )
改为
if args.use_ema:
                ema_writer = SummaryWriter(os.path.join(args.output_dir, 'ema_eval'))
                ema_test_stats, _ = evaluate(
                    self.ema_m.module, criterion, postprocessors, data_loader_val, base_ds, device, args=args, writer=ema_writer, epoch=epoch
                )
                ema_writer.close()

if __name__ == '__main__':
1767行
writer = SummaryWriter('runs')
改为
writer = SummaryWriter(args.output_dir)

24.注释微调部分
# from peft import LoraConfig, get_peft_model

25.rfdetr\models\backbone\backbone.py
# from peft import LoraConfig, get_peft_model, PeftModel

26.rfdetr\models\backbone\dinov2_with_windowed_attn.py
28行去除torch_int,
261行class WindowedDinov2WithRegistersEmbeddings(nn.Module):
def interpolate_pos_encoding

sqrt_num_positions = torch_int(num_positions**0.5)
改为
sqrt_num_positions = int(num_positions**0.5)

271行
size=(torch_int(height), torch_int(width)),  # Explicit size instead of scale_factor
改为
size=(int(height), int(width)),  # Explicit size instead of scale_factor

598行class WindowedDinov2WithRegistersLayer(nn.Module):添加
config._attn_implementation = "eager"

27.1633行parser.add_argument('--batch_size', default=2, type=int)
default=2改为1

28.修改num_classes参数以匹配数据集类别数(10个类别)
#    - rfdetr/main.py第1629行: parser.add_argument('--num_classes', default=10, type=int)
#    - rfdetr/config.py第27行: num_classes: int = 10

29.rfdetr/config.py - 将num_classes从10改为11
rfdetr/main.py - 在populate_args函数中将num_classes默认值从10改为11
rfdetr/main.py - 在命令行参数解析器中将num_classes默认值从10改为11

30根据数据集将rfdetr/config.py与rfdetr/main.py的num_classes改为10
31.rfdetr/engine.py的train_one_epoch函数中添加训练损失记录到tensorboard
Training time 9:20:12
Results saved to output\results.json
32. 在rfdetr/engine.py的save_detection_images函数中，将`text = f"L:{label.item()} S:{score.item():.2f}"`改为`text = f"{category_name} {score.item():.2f}"`。
33. 在rfdetr/engine.py的evaluate函数中，将
    if writer and epoch is not None:
        # Log all stats from metric_logger
        for k, meter in metric_logger.meters.items():
            if k != 'coco_eval_bbox': # Exclude coco_eval_bbox as it's a list
                writer.add_scalar(f'eval/{k}', meter.global_avg, epoch)
        if 'fps' in stats:
            writer.add_scalar('eval/FPS', stats['fps'], epoch)
    改为
    if writer and epoch is not None:
        for k, v in stats.items():
            if isinstance(v, (int, float)):
                writer.add_scalar(f'eval/{k}', v, epoch)
34. 在rfdetr/engine.py的save_detection_images函数中，将`category_name = coco_dataset.cats[category_id]['name'] if category_id in coco_dataset.cats else f"ID:{category_id}"`改为`category_name = {1: "person", 2: "rider", 3: "car", 4: "bus", 5: "truck", 6: "bike", 7: "motor", 8: "traffic light", 9: "traffic sign", 10: "train"}.get(category_id, f"ID:{category_id}")`。
35. 在rfdetr/engine.py的evaluate函数中，将
    if writer and epoch is not None:
        for k, v in stats.items():
            if isinstance(v, (int, float)):
                writer.add_scalar(f'eval/{k}', v, epoch)
    改为
    if writer and epoch is not None:
        writer.add_scalar('eval/loss', stats['loss'], epoch)
        writer.add_scalar('eval/class_error', stats['class_error'], epoch)
        writer.add_scalar('eval/mAP@.50-.95', stats['mAP@.50-.95'], epoch)
        writer.add_scalar('eval/mAP@.50', stats['mAP@.50'], epoch)
        writer.add_scalar('eval/FPS', stats['fps'], epoch)

36. 在rfdetr/engine.py的evaluate函数中，将`print(f"FPS (batch_size=1): {fps:.2f}")`改为`print(f"FPS (batch_size=1): {fps:.2f}")`并在TensorBoard中记录FPS。
37. 在rfdetr/engine.py的save_detection_images函数中，添加`sys.stdout.flush()`以确保立即打印类别名称。
38. 在 `rfdetr/engine.py` 的 `save_detection_images` 函数中进行了以下修改以解决可视化问题：
    - 在文件顶部导入 `ImageFont`。
    - 添加了字体加载逻辑，优先使用 "arial.ttf"，失败则回退到默认字体。
    - 将硬编码的类别名称改为从 `coco_dataset` 动态获取。
    - 修复了在旧版 Pillow 的回退路径中 `draw.textsize` 和 `draw.text` 未传递 `font` 对象的问题。
    - 优化了标签文本的位置和背景，以提高可读性。
    - 增加了在找不到原始带标注图像时，从张量重建图像的回退功能。
39. 在 `rfdetr/engine.py` 的 `evaluate` 函数中，为 TensorBoard 日志记录添加了健壮性检查：
    - 在尝试使用 `writer.add_scalar` 记录 'class_error', 'mAP@.50-.95', 'mAP@.50', 和 'FPS' 之前，先检查这些键是否存在于 `stats` 字典中。
    - 如果某个键不存在，则会打印一条警告信息，而不是引发错误，从而避免程序中断并帮助诊断问题。

40. 在文件 'rfdetr\main.py' 中: ## 变更摘要

本次变更主要是对`rfdetr/main.py`文件进行了精简和参数调整：

41. **删除了大量历史修改注释**：移除了文件开头的详细修改日志（v7版本修改说明），该日志包含了之前解决tensorboard中map曲线显示问题的各种修改记录。

42. **调整了模型类别数量**：
   - 将`populate_args`函数中的`num_classes`参数默认值从2改为10
   - 将`get_args_parser`函数中的`--num_classes`参数默认值从2改为10

43. **保留了核心代码结构**：维持了原有的Model类定义、参数解析器和主要导入语句不变，仅删除了不必要的注释和空行。

这次变更主要是为了适应多类别检测任务（从2类扩展到10类），同时通过删除冗长的历史注释使代码更加简洁。

44. 在文件 'rfdetr\config.py' 中: 本次变更修改了 `rfdetr/config.py` 文件中的 `ModelConfig` 类，将 `num_classes` 参数的默认值从 90 调整为 10。这个参数用于指定模型处理的类别数量，变更表明模型默认处理的类别数量有所减少，可能是为了适应特定场景或简化模型复杂度。

45. 在文件 'rfdetr\main.py' 中: 本次变更修改了模型加载预训练权重时的类别不匹配提示信息。主要变化包括：

将日志级别从 `warning` 改为 `info`，表明这不再是警告信息而是常规信息提示
重写了提示文本，使其更加清晰友好：
   - 明确指出这是"使用不同类别数进行微调"的场景
   - 更清晰地展示预训练模型的类别数和当前配置的类别数
   - 明确说明检测头将被重新初始化以匹配当前数据集

这些修改使得用户在加载预训练权重进行微调时能获得更清晰、更友好的提示信息，减少不必要的担忧。

46. 在文件 'rfdetr\engine.py' 中: 本次代码变更为rfdetr/engine.py文件中save_detection_images函数内的类别映射获取逻辑优化。变更简化了从数据集中获取类别名称的方式，由原来的手动构建categories字典改为直接使用coco_dataset对象的cats属性。这种修改使代码更加简洁，同时通过条件判断确保了对未知类别ID的容错处理。

40. 在 `rfdetr/engine.py` 的 `save_detection_images` 函数中，进一步优化了字体加载逻辑：
    - 将字体路径明确指定为 `C:/Windows/Fonts/arial.ttf`，以提高在 Windows 系统上的加载成功率。
    - 将字体大小增加到 20，以确保标签文本在各种分辨率下都清晰可见。
    - 添加了调试信息，用于在加载字体时打印成功或失败的状态。

41. 在 `rfdetr/engine.py` 的 `save_detection_images` 函数中，为彻底解决部分标签仍显示为 ID 的问题，将动态获取类别名称的逻辑:
    `category_name = coco_dataset.cats[category_id]['name'] if category_id in coco_dataset.cats else f"ID:{category_id}"`
    替换为使用硬编码的类别字典:
    `category_mapping = {1: "person", 2: "rider", 3: "car", 4: "bus", 5: "truck", 6: "bike", 7: "motor", 8: "traffic light", 9: "traffic sign", 10: "train"}`
    `category_name = category_mapping.get(category_id, f"ID:{category_id}")`

42. 修复可视化不显示文本标签问题：
    - 增强字体加载逻辑，添加多个字体选项作为备选，包括中文字体（黑体、微软雅黑）
    - 增加字体大小从20到24，提高标签可见性
    - 改进文本位置计算，确保标签不会超出图像边界
    - 优化文本绘制逻辑，添加红色背景框和白色文字以提高对比度
    - 添加调试输出，记录绘制的标签数量和内容
    - 添加sys.stdout.flush()确保立即输出调试信息

43. 修复tensorboard不显示fps与map问题：
    - 在FPS计算后立即记录到TensorBoard，确保数据不会丢失
    - 增强mAP值提取的错误处理，确保coco_eval_bbox列表长度足够
    - 添加详细的调试输出，记录FPS和mAP值以及它们是否成功记录到TensorBoard
    - 在所有指标记录后调用writer.flush()，确保数据立即写入TensorBoard日志
    - 改进错误处理和警告信息，帮助诊断潜在问题

44. 进一步修复可视化不显示文本标签问题：
    - 优先从coco_dataset中获取类别名称，而不是直接使用硬编码映射
    - 添加详细的调试输出，显示可用的类别和映射关系
    - 打印每个检测到的类别ID和对应的类别名称，便于诊断问题
    - 增强文本绘制过程的调试信息，包括文本位置和绘制结果
    - 确保在找不到类别名称时提供有意义的回退方案

47. 在文件 'rfdetr\main.py' 中: 这个变更摘要如下：

从 `rfdetr/main.py` 文件中移除了对 `argparse` 模块的导入。该变更位于文件顶部的导入部分，删除了 `import argparse` 这一行，但保留了其他导入语句如 `ast`、`copy` 和 `datetime` 等。这可能意味着代码中不再需要使用命令行参数解析功能，或者该功能将被其他方式替代。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 255da4b..9e672dd 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -22,7 +22,7 @@ project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
 if project_root not in sys.path:
     sys.path.insert(0, project_root)
 
-import argparse
+
 import ast
 import copy
 import datetime

--- End Diff ---

48. 在文件 'rfdetr\main.py' 中: ## 代码变更摘要

在`rfdetr/main.py`文件中添加了`argparse`模块的导入语句。这一变更将为程序添加命令行参数解析功能，使其能够从命令行接收和处理参数。这是一个基础性的修改，为后续可能的命令行交互功能做准备。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 9e672dd..255da4b 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -22,7 +22,7 @@ project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
 if project_root not in sys.path:
     sys.path.insert(0, project_root)
 
-
+import argparse
 import ast
 import copy
 import datetime

--- End Diff ---

49. 在文件 'rfdetr\engine.py' 中: # 变更摘要：增强评估函数与可视化功能

本次变更主要针对`rfdetr/engine.py`文件中的评估和可视化功能进行了全面增强，核心内容包括：

## 1. FPS计算与日志记录增强
- 重复了FPS计算代码（可能为临时测试）
- 添加了更详细的TensorBoard日志记录和打印输出
- 新增`writer.flush()`确保TensorBoard写入立即执行

## 2. mAP指标提取的健壮性改进
- 增加了对`coco_eval_bbox`列表长度的检查，防止索引越界错误
- 添加了mAP值的打印输出和警告信息
- 改进了TensorBoard日志记录的错误处理机制

## 3. 图像保存与可视化功能重大改进
- **字体加载增强**：
  - 支持多种Windows字体路径尝试（包括中文字体如黑体、微软雅黑）
  - 增大字体尺寸从15到24，提高可见性
  - 添加字体加载失败的详细日志

- **类别映射改进**：
  - 优先从coco_dataset获取类别名称
  - 添加硬编码类别映射作为备选方案
  - 增加详细的类别映射调试信息

- **文本绘制增强**：
  - 改进文本位置计算逻辑，确保标签始终可见
  - 添加文本背景以提高可读性
  - 增加对不同Pillow版本的兼容性处理
  - 添加绘制框计数和详细的绘制过程日志

- **错误处理与调试**：
  - 添加多层错误处理和回退机制
  - 增加大量调试输出，便于问题定位
  - 添加`sys.stdout.flush()`确保日志立即输出

这些变更显著提高了评估过程的健壮性和可视化效果，使模型评估结果更加清晰、可靠，并便于调试分析。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index f1c4589..2e780db 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -293,8 +293,19 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, arg
         fps = 0
     
     print(f"FPS (batch_size=1): {fps:.2f}")
+    # Calculate FPS
+    if forward_pass_times:
+        avg_forward_time = np.mean(forward_pass_times)
+        fps = 1 / avg_forward_time
+    else:
+        fps = 0
+    
+    print(f"FPS (batch_size=1): {fps:.2f}")
+    
+    # Log FPS to TensorBoard immediately after calculation
     if writer and epoch is not None:
         writer.add_scalar('eval/FPS', fps, epoch)
+        print(f"Logged FPS to TensorBoard: {fps:.2f}")
 
     stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
     stats['fps'] = fps
@@ -302,28 +313,43 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, arg
     if coco_evaluator is not None:
         if "bbox" in postprocessors.keys():
             stats["coco_eval_bbox"] = coco_evaluator.coco_eval["bbox"].stats.tolist()
-            stats['mAP@.50-.95'] = stats["coco_eval_bbox"][0]
-            stats['mAP@.50'] = stats["coco_eval_bbox"][1]
+            # Ensure mAP values are properly extracted
+            if len(stats["coco_eval_bbox"]) > 1:
+                stats['mAP@.50-.95'] = stats["coco_eval_bbox"][0]
+                stats['mAP@.50'] = stats["coco_eval_bbox"][1]
+                print(f"mAP@.50-.95: {stats['mAP@.50-.95']:.4f}")
+                print(f"mAP@.50: {stats['mAP@.50']:.4f}")
+            else:
+                print("Warning: coco_eval_bbox stats list is too short.")
 
     if writer and epoch is not None:
+        # Log basic metrics
         writer.add_scalar('eval/loss', stats['loss'], epoch)
         if 'class_error' in stats:
             writer.add_scalar('eval/class_error', stats['class_error'], epoch)
         
+        # Log mAP metrics with proper error handling
         if 'mAP@.50-.95' in stats:
             writer.add_scalar('eval/mAP@.50-.95', stats['mAP@.50-.95'], epoch)
+            print(f"Logged mAP@.50-.95 to TensorBoard: {stats['mAP@.50-.95']:.4f}")
         else:
             print("Warning: 'mAP@.50-.95' not found in stats. Skipping TensorBoard log.")
             
         if 'mAP@.50' in stats:
             writer.add_scalar('eval/mAP@.50', stats['mAP@.50'], epoch)
+            print(f"Logged mAP@.50 to TensorBoard: {stats['mAP@.50']:.4f}")
         else:
             print("Warning: 'mAP@.50' not found in stats. Skipping TensorBoard log.")
 
+        # Log FPS again to ensure it's recorded
         if 'fps' in stats:
             writer.add_scalar('eval/FPS', stats['fps'], epoch)
+            print(f"Logged FPS to TensorBoard (from stats): {stats['fps']:.2f}")
         else:
             print("Warning: 'fps' not found in stats. Skipping TensorBoard log.")
+        
+        # Ensure TensorBoard writes are flushed
+        writer.flush()
 
     if coco_evaluator and "segm" in postprocessors.keys():
         stats["coco_eval_masks"] = coco_evaluator.coco_eval["segm"].stats.tolist()
@@ -365,12 +391,28 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
 
     draw = ImageDraw.Draw(img)
 
-    # Load font
-    try:
-        font = ImageFont.truetype("arial.ttf", 15)
-    except IOError:
-        print("Arial font not found. Falling back to default font.")
+    # Load font with multiple fallback options
+    font_paths = [
+        "C:/Windows/Fonts/arial.ttf",
+        "C:/Windows/Fonts/simhei.ttf",  # 黑体
+        "C:/Windows/Fonts/msyh.ttf",    # 微软雅黑
+        "C:/Windows/Fonts/tahoma.ttf"
+    ]
+    font_size = 24  # Increased font size for better visibility
+    font = None
+    
+    for font_path in font_paths:
+        try:
+            font = ImageFont.truetype(font_path, font_size)
+            print(f"Successfully loaded font from {font_path}")
+            break
+        except IOError:
+            print(f"Warning: Font not found at {font_path}. Trying next option...")
+    
+    if font is None:
+        print("Warning: No truetype font found. Falling back to default font.")
         font = ImageFont.load_default()
+        font_size = 12  # Default font is usually smaller
 
     # Get original size for scaling boxes
     orig_size = target['orig_size']
@@ -382,6 +424,9 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     os.makedirs(img_output_dir, exist_ok=True)
     print(f"Saving image to directory: {img_output_dir}")
 
+    # Count of drawn boxes for debugging
+    drawn_boxes = 0
+
     if criterion:
         # Use matcher to find corresponding predicted boxes
         indices = criterion.matcher(outputs, targets)
@@ -396,31 +441,91 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
             scaled_pred_boxes = box_ops.box_cxcywh_to_xyxy(pred_boxes)
             scaled_pred_boxes = scaled_pred_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_pred_boxes.device)
             
+            # Try to get category names from coco_dataset first
+            category_mapping = {}
+            if coco_dataset and hasattr(coco_dataset, 'cats'):
+                print(f"Available categories in coco_dataset: {coco_dataset.cats}")
+                for cat_id, cat_info in coco_dataset.cats.items():
+                    category_mapping[cat_id] = cat_info['name']
+                    print(f"Added category mapping: {cat_id} -> {cat_info['name']}")
+            
+            # If no categories found in coco_dataset, use hardcoded mapping
+            if not category_mapping:
+                print("No categories found in coco_dataset, using hardcoded mapping")
+                category_mapping = {
+                    1: "person", 2: "rider", 3: "car", 4: "bus", 5: "truck",
+                    6: "bike", 7: "motor", 8: "traffic light", 9: "traffic sign", 10: "train"
+                }
+            
+            print(f"Final category mapping: {category_mapping}")
+            
             for box, label, score in zip(scaled_pred_boxes, labels, scores):
                 box = box.cpu().tolist()
                 category_id = label.item()
-                category_name = coco_dataset.cats[category_id]['name'] if category_id in coco_dataset.cats else f"ID:{category_id}"
+                print(f"Processing box with category_id: {category_id}")
                 
+                # Get category name with detailed debugging
+                if category_id in category_mapping:
+                    category_name = category_mapping[category_id]
+                    print(f"Found category name in mapping: {category_name}")
+                else:
+                    category_name = f"ID:{category_id}"
+                    print(f"Category ID {category_id} not found in mapping, using ID format")
+                
+                # Draw bounding box
                 draw.rectangle(box, outline="red", width=2)
                 
+                # Prepare text
                 text = f"{category_name} {score.item():.2f}"
-                text_position = (box[0], box[1] - 15 if box[1] - 15 > 0 else box[1])
+                print(f"Text to draw: {text}")
+                
+                # Calculate text position with better visibility
+                text_x = box[0] + 2
+                text_y = box[1] - font_size - 2  # Position above the box
                 
+                # If text would be above image, position it below the top of the box
+                if text_y < 0:
+                    text_y = box[1] + 2
+                
+                text_position = (text_x, text_y)
+                print(f"Text position: {text_position}")
+                
+                # Try to draw text with background for better visibility
                 try:
                     # Use textbbox for modern Pillow versions
                     text_bbox = draw.textbbox(text_position, text, font=font)
-                    # Adjust bbox to add some padding
-                    padded_bbox = (text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2)
-                    draw.rectangle(padded_bbox, fill="red")
+                    print(f"Text bbox: {text_bbox}")
+                    # Draw background rectangle
+                    draw.rectangle(text_bbox, fill="red")
+                    # Draw text in white for contrast
                     draw.text(text_position, text, fill="white", font=font)
+                    drawn_boxes += 1
+                    print(f"Successfully drew text with background")
                 except AttributeError:
                     # Fallback for older Pillow versions
-                    text_size = draw.textsize(text, font=font)
-                    draw.rectangle(
-                        (text_position[0], text_position[1], text_position[0] + text_size[0], text_position[1] + text_size[1]),
-                        fill="red"
-                    )
-                    draw.text(text_position, text, fill='white', font=font)
+                    try:
+                        text_size = draw.textsize(text, font=font)
+                        print(f"Text size: {text_size}")
+                        # Draw background rectangle
+                        draw.rectangle(
+                            (text_position[0], text_position[1],
+                             text_position[0] + text_size[0], text_position[1] + text_size[1]),
+                            fill="red"
+                        )
+                        # Draw text in white for contrast
+                        draw.text(text_position, text, fill='white', font=font)
+                        drawn_boxes += 1
+                        print(f"Successfully drew text with fallback method")
+                    except Exception as e:
+                        print(f"Error drawing text: {e}")
+                        # Last resort: draw text without background
+                        draw.text(text_position, text, fill='red', font=font)
+                        drawn_boxes += 1
+                        print(f"Drew text without background due to error")
+                
+                print(f"Drawn box {drawn_boxes}: {category_name} at {box} with score {score.item():.2f}")
+        else:
+            print("No predicted boxes to draw.")
     else:
         print("Warning: `criterion` not provided to `save_detection_images`. Cannot determine which boxes are used for loss. Skipping image save.")
         return
@@ -428,3 +533,5 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     # Save the modified image
     save_path = os.path.join(img_output_dir, f"epoch_{epoch}.png")
     img.save(save_path)
+    print(f"Image saved to {save_path} with {drawn_boxes} labeled boxes")
+    sys.stdout.flush()  # Ensure immediate output

--- End Diff ---

50. 在文件 'rfdetr\engine.py' 中: ## 代码变更摘要

本次变更主要针对`rfdetr/engine.py`文件中的字体加载逻辑进行了优化和增强，以提高跨平台兼容性和错误处理能力。

### 核心变更内容：

1. **字体路径扩展**：
   - 新增了Linux和macOS系统的字体路径，增强了跨平台兼容性
   - 将微软雅黑字体路径从`.ttf`修正为`.ttc`（更准确的文件扩展名）

2. **字体加载逻辑优化**：
   - 在尝试加载字体前，先检查文件是否存在（`os.path.exists`）
   - 改进了异常处理，同时捕获`IOError`和`OSError`，并输出具体错误信息

3. **字体大小调整**：
   - 将字体大小从24调整为20，提供更适中的显示效果
   - 优化了默认字体的回退逻辑，在无法加载TrueType字体时尝试使用更大的默认字体尺寸

4. **错误处理增强**：
   - 增加了更详细的错误日志输出
   - 提供了多层次的回退方案，确保在字体加载失败时仍能正常工作

这些变更提高了代码的健壮性和跨平台兼容性，确保在不同操作系统上都能正确加载和显示字体，同时优化了字体大小以获得更好的可视化效果。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 2e780db..316b3f3 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -391,28 +391,37 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
 
     draw = ImageDraw.Draw(img)
 
-    # Load font with multiple fallback options
+    # Load font with multiple fallback options and better error处理
     font_paths = [
         "C:/Windows/Fonts/arial.ttf",
         "C:/Windows/Fonts/simhei.ttf",  # 黑体
-        "C:/Windows/Fonts/msyh.ttf",    # 微软雅黑
-        "C:/Windows/Fonts/tahoma.ttf"
+        "C:/Windows/Fonts/msyh.ttc",    # 微软雅黑
+        "C:/Windows/Fonts/tahoma.ttf",
+        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",  # Linux
+        "/System/Library/Fonts/Helvetica.ttc",  # macOS
     ]
-    font_size = 24  # Increased font size for better visibility
+    font_size = 20  # 适中的字体大小
     font = None
     
     for font_path in font_paths:
         try:
-            font = ImageFont.truetype(font_path, font_size)
-            print(f"Successfully loaded font from {font_path}")
-            break
-        except IOError:
-            print(f"Warning: Font not found at {font_path}. Trying next option...")
+            if os.path.exists(font_path):
+                font = ImageFont.truetype(font_path, font_size)
+                print(f"Successfully loaded font from {font_path}")
+                break
+        except (IOError, OSError) as e:
+            print(f"Warning: Font not found at {font_path}: {e}")
     
     if font is None:
-        print("Warning: No truetype font found. Falling back to default font.")
-        font = ImageFont.load_default()
-        font_size = 12  # Default font is usually smaller
+        print("Warning: No truetype font found. Using default font with larger size.")
+        try:
+            # 尝试使用Pillow的默认字体，但增大尺寸
+            font = ImageFont.load_default()
+            font_size = 16
+        except:
+            # 最后的备选方案
+            font = ImageFont.load_default()
+            font_size = 12
 
     # Get original size for scaling boxes
     orig_size = target['orig_size']

--- End Diff ---

51. 在文件 'rfdetr\engine.py' 中: # 变更摘要：改进目标检测图像保存功能

本次变更对`rfdetr/engine.py`中的`save_detection_images`函数进行了全面优化，主要改进包括：

1. **类别名称获取逻辑增强**：
   - 实现了多级回退机制，依次尝试从coco_dataset、COCO_CLASSES模块和硬编码映射获取类别信息
   - 增加了完善的异常处理，确保在任何情况下都能提供类别名称

2. **预测结果过滤**：
   - 新增0.3的置信度阈值，过滤低置信度预测，提高可视化质量

3. **边界框和文本显示优化**：
   - 确保边界框坐标在图像范围内，防止越界
   - 改进文本位置计算，确保标签始终可见
   - 增强文本背景框绘制逻辑，兼容新旧版本Pillow库
   - 边界框线条加粗(从2到3)，提高可见性

4. **错误处理和代码健壮性**：
   - 添加多层try-except结构，提高代码容错能力
   - 移除冗余调试输出，保留关键错误信息

这些改进使目标检测可视化结果更加清晰、稳定，并增强了代码在各种环境下的适应性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 316b3f3..155beae 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -450,89 +450,116 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
             scaled_pred_boxes = box_ops.box_cxcywh_to_xyxy(pred_boxes)
             scaled_pred_boxes = scaled_pred_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_pred_boxes.device)
             
-            # Try to get category names from coco_dataset first
+            # 改进的类别名称获取逻辑
             category_mapping = {}
-            if coco_dataset and hasattr(coco_dataset, 'cats'):
-                print(f"Available categories in coco_dataset: {coco_dataset.cats}")
-                for cat_id, cat_info in coco_dataset.cats.items():
-                    category_mapping[cat_id] = cat_info['name']
-                    print(f"Added category mapping: {cat_id} -> {cat_info['name']}")
-            
-            # If no categories found in coco_dataset, use hardcoded mapping
-            if not category_mapping:
-                print("No categories found in coco_dataset, using hardcoded mapping")
+            try:
+                # 从coco_dataset获取类别映射
+                if coco_dataset and hasattr(coco_dataset, 'cats'):
+                    for cat_id, cat_info in coco_dataset.cats.items():
+                        category_mapping[cat_id] = cat_info['name']
+                
+                # 如果coco_dataset没有类别信息，尝试从util获取
+                if not category_mapping:
+                    try:
+                        from rfdetr.util.coco_classes import COCO_CLASSES
+                        category_mapping = {i+1: name for i, name in enumerate(COCO_CLASSES)}
+                        print(f"Using COCO_CLASSES mapping: {len(category_mapping)} classes")
+                    except ImportError:
+                        print("COCO_CLASSES not found, using basic mapping")
+                        category_mapping = {
+                            1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 5: "airplane",
+                            6: "bus", 7: "train", 8: "truck", 9: "boat", 10: "traffic light"
+                        }
+                
+            except Exception as e:
+                print(f"Error getting category mapping: {e}, using fallback")
                 category_mapping = {
-                    1: "person", 2: "rider", 3: "car", 4: "bus", 5: "truck",
-                    6: "bike", 7: "motor", 8: "traffic light", 9: "traffic sign", 10: "train"
+                    1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 5: "airplane",
+                    6: "bus", 7: "train", 8: "truck", 9: "boat", 10: "traffic light"
                 }
             
-            print(f"Final category mapping: {category_mapping}")
+            # 过滤低置信度预测
+            confidence_threshold = 0.3
             
             for box, label, score in zip(scaled_pred_boxes, labels, scores):
+                score_value = score.item()
+                if score_value < confidence_threshold:
+                    continue
+                    
                 box = box.cpu().tolist()
                 category_id = label.item()
-                print(f"Processing box with category_id: {category_id}")
                 
-                # Get category name with detailed debugging
-                if category_id in category_mapping:
-                    category_name = category_mapping[category_id]
-                    print(f"Found category name in mapping: {category_name}")
-                else:
-                    category_name = f"ID:{category_id}"
-                    print(f"Category ID {category_id} not found in mapping, using ID format")
+                # 获取类别名称
+                category_name = category_mapping.get(category_id, f"Class_{category_id}")
                 
-                # Draw bounding box
-                draw.rectangle(box, outline="red", width=2)
+                # 确保box坐标在图像范围内
+                box = [
+                    max(0, min(box[0], img_w)),
+                    max(0, min(box[1], img_h)),
+                    max(0, min(box[2], img_w)),
+                    max(0, min(box[3], img_h))
+                ]
                 
-                # Prepare text
-                text = f"{category_name} {score.item():.2f}"
-                print(f"Text to draw: {text}")
+                # 绘制边界框
+                draw.rectangle(box, outline="red", width=3)
                 
-                # Calculate text position with better visibility
-                text_x = box[0] + 2
-                text_y = box[1] - font_size - 2  # Position above the box
+                # 准备文本
+                text = f"{category_name}: {score_value:.2f}"
                 
-                # If text would be above image, position it below the top of the box
-                if text_y < 0:
-                    text_y = box[1] + 2
+                # 计算文本位置 - 确保在图像内
+                text_x = max(2, min(box[0] + 2, img_w - 100))
+                text_y = max(2, box[1] - font_size - 2)
+                
+                # 如果文本会超出上边界，放在框内
+                if text_y < 2:
+                    text_y = max(2, box[1] + 2)
                 
                 text_position = (text_x, text_y)
-                print(f"Text position: {text_position}")
                 
-                # Try to draw text with background for better visibility
+                # 绘制带背景的文本
                 try:
-                    # Use textbbox for modern Pillow versions
-                    text_bbox = draw.textbbox(text_position, text, font=font)
-                    print(f"Text bbox: {text_bbox}")
-                    # Draw background rectangle
-                    draw.rectangle(text_bbox, fill="red")
-                    # Draw text in white for contrast
+                    # 获取文本边界框
+                    if hasattr(draw, 'textbbox'):
+                        text_bbox = draw.textbbox(text_position, text, font=font)
+                        text_width = text_bbox[2] - text_bbox[0]
+                        text_height = text_bbox[3] - text_bbox[1]
+                    else:
+                        # 旧版本Pillow的备选方案
+                        text_width = len(text) * font_size * 0.6
+                        text_height = font_size
+                        text_bbox = (text_position[0], text_position[1],
+                                   text_position[0] + text_width, text_position[1] + text_height)
+                    
+                    # 调整背景框大小
+                    padding = 2
+                    bg_box = [
+                        text_bbox[0] - padding,
+                        text_bbox[1] - padding,
+                        text_bbox[2] + padding,
+                        text_bbox[3] + padding
+                    ]
+                    
+                    # 确保背景框在图像内
+                    bg_box[0] = max(0, bg_box[0])
+                    bg_box[1] = max(0, bg_box[1])
+                    bg_box[2] = min(img_w, bg_box[2])
+                    bg_box[3] = min(img_h, bg_box[3])
+                    
+                    # 绘制背景
+                    draw.rectangle(bg_box, fill="red")
+                    
+                    # 绘制文本
                     draw.text(text_position, text, fill="white", font=font)
                     drawn_boxes += 1
-                    print(f"Successfully drew text with background")
-                except AttributeError:
-                    # Fallback for older Pillow versions
+                    
+                except Exception as e:
+                    print(f"Error drawing text for {category_name}: {e}")
+                    # 最后的备选：直接绘制文本
                     try:
-                        text_size = draw.textsize(text, font=font)
-                        print(f"Text size: {text_size}")
-                        # Draw background rectangle
-                        draw.rectangle(
-                            (text_position[0], text_position[1],
-                             text_position[0] + text_size[0], text_position[1] + text_size[1]),
-                            fill="red"
-                        )
-                        # Draw text in white for contrast
-                        draw.text(text_position, text, fill='white', font=font)
-                        drawn_boxes += 1
-                        print(f"Successfully drew text with fallback method")
-                    except Exception as e:
-                        print(f"Error drawing text: {e}")
-                        # Last resort: draw text without background
-                        draw.text(text_position, text, fill='red', font=font)
+                        draw.text(text_position, text, fill="red", font=font)
                         drawn_boxes += 1
-                        print(f"Drew text without background due to error")
-                
-                print(f"Drawn box {drawn_boxes}: {category_name} at {box} with score {score.item():.2f}")
+                    except:
+                        pass
         else:
             print("No predicted boxes to draw.")
     else:

--- End Diff ---

52. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

对`rfdetr/engine.py`中的`evaluate`函数进行了以下优化：

1. **移除重复代码**：删除了重复的FPS计算和打印逻辑，避免冗余操作。

2. **增强mAP指标记录**：添加了从COCO评估器中提取和记录多种mAP指标的功能，包括：
   - mAP@.50-.95、mAP@.50、mAP@.75
   - 不同物体尺寸(small, medium, large)的mAP值
   - 增加了错误处理机制，确保在评估数据不完整时仍能记录基本指标

3. **移除TensorBoard记录**：删除了将FPS记录到TensorBoard的相关代码。

4. **代码结构问题**：stats字典被构建了两次，可能导致mAP指标被覆盖，需要进一步修复。

此次变更主要优化了评估函数的指标记录功能，提高了代码的健壮性，但引入了stats字典重复构建的问题。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 155beae..134b54e 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -285,7 +285,7 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, arg
         coco_evaluator.accumulate()
         coco_evaluator.summarize()
 
-    # Calculate FPS
+    # Calculate FPS - 移除重复计算
     if forward_pass_times:
         avg_forward_time = np.mean(forward_pass_times)
         fps = 1 / avg_forward_time
@@ -293,19 +293,33 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, arg
         fps = 0
     
     print(f"FPS (batch_size=1): {fps:.2f}")
-    # Calculate FPS
-    if forward_pass_times:
-        avg_forward_time = np.mean(forward_pass_times)
-        fps = 1 / avg_forward_time
-    else:
-        fps = 0
     
-    print(f"FPS (batch_size=1): {fps:.2f}")
+    # 确保FPS被正确记录到stats中
+    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
+    stats['fps'] = fps
     
-    # Log FPS to TensorBoard immediately after calculation
-    if writer and epoch is not None:
-        writer.add_scalar('eval/FPS', fps, epoch)
-        print(f"Logged FPS to TensorBoard: {fps:.2f}")
+    # 确保mAP值被正确提取和记录
+    if coco_evaluator is not None:
+        if "bbox" in postprocessors.keys():
+            coco_stats = coco_evaluator.coco_eval["bbox"].stats.tolist()
+            stats["coco_eval_bbox"] = coco_stats
+            
+            # 确保mAP值存在
+            if len(coco_stats) >= 12:  # COCO评估有12个指标
+                stats['mAP@.50-.95'] = float(coco_stats[0])
+                stats['mAP@.50'] = float(coco_stats[1])
+                stats['mAP@.75'] = float(coco_stats[2])
+                stats['mAP_small'] = float(coco_stats[3])
+                stats['mAP_medium'] = float(coco_stats[4])
+                stats['mAP_large'] = float(coco_stats[5])
+                print(f"mAP@.50-.95: {stats['mAP@.50-.95']:.4f}")
+                print(f"mAP@.50: {stats['mAP@.50']:.4f}")
+                print(f"mAP@.75: {stats['mAP@.75']:.4f}")
+            else:
+                print("Warning: coco_eval_bbox stats list is incomplete")
+                # 至少设置基本的mAP值
+                stats['mAP@.50-.95'] = float(coco_stats[0]) if len(coco_stats) > 0 else 0.0
+                stats['mAP@.50'] = float(coco_stats[1]) if len(coco_stats) > 1 else 0.0
 
     stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
     stats['fps'] = fps

--- End Diff ---

53. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

改进了`rfdetr/engine.py`中`evaluate`函数的TensorBoard日志记录功能，主要变更包括：

1. **增强日志记录健壮性**：将条件检查方式改为使用`dict.get()`提供默认值，确保即使指标缺失也能继续记录，避免因指标不存在而跳过记录的情况。

2. **扩展记录指标范围**：
   - 新增`mAP@.75`指标记录
   - 添加目标尺寸细分的mAP指标：`mAP_small`、`mAP_medium`、`mAP_large`

3. **改进日志输出**：增加更详细的日志信息，包括记录的具体指标值和确认消息，便于调试和监控记录过程。

4. **代码结构优化**：重构了FPS和mAP指标的记录逻辑，使代码更简洁且功能更全面。

此变更确保了评估过程中的关键性能指标能够被完整记录到TensorBoard，提高了模型性能监控的全面性和可靠性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 134b54e..3bbec3d 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -337,33 +337,44 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, arg
                 print("Warning: coco_eval_bbox stats list is too short.")
 
     if writer and epoch is not None:
-        # Log basic metrics
+        # 确保所有重要指标都被记录到TensorBoard
+        print(f"Logging metrics to TensorBoard for epoch {epoch}")
+        
+        # 基本指标
         writer.add_scalar('eval/loss', stats['loss'], epoch)
         if 'class_error' in stats:
             writer.add_scalar('eval/class_error', stats['class_error'], epoch)
         
-        # Log mAP metrics with proper error handling
-        if 'mAP@.50-.95' in stats:
-            writer.add_scalar('eval/mAP@.50-.95', stats['mAP@.50-.95'], epoch)
-            print(f"Logged mAP@.50-.95 to TensorBoard: {stats['mAP@.50-.95']:.4f}")
-        else:
-            print("Warning: 'mAP@.50-.95' not found in stats. Skipping TensorBoard log.")
-            
-        if 'mAP@.50' in stats:
-            writer.add_scalar('eval/mAP@.50', stats['mAP@.50'], epoch)
-            print(f"Logged mAP@.50 to TensorBoard: {stats['mAP@.50']:.4f}")
-        else:
-            print("Warning: 'mAP@.50' not found in stats. Skipping TensorBoard log.")
-
-        # Log FPS again to ensure it's recorded
-        if 'fps' in stats:
-            writer.add_scalar('eval/FPS', stats['fps'], epoch)
-            print(f"Logged FPS to TensorBoard (from stats): {stats['fps']:.2f}")
-        else:
-            print("Warning: 'fps' not found in stats. Skipping TensorBoard log.")
+        # FPS指标 - 确保记录
+        fps_value = stats.get('fps', 0.0)
+        writer.add_scalar('eval/FPS', fps_value, epoch)
+        print(f"Logged FPS to TensorBoard: {fps_value:.2f}")
+        
+        # mAP指标 - 确保记录
+        map_5095 = stats.get('mAP@.50-.95', 0.0)
+        map_50 = stats.get('mAP@.50', 0.0)
+        map_75 = stats.get('mAP@.75', 0.0)
+        
+        writer.add_scalar('eval/mAP@.50-.95', map_5095, epoch)
+        writer.add_scalar('eval/mAP@.50', map_50, epoch)
+        writer.add_scalar('eval/mAP@.75', map_75, epoch)
+        
+        print(f"Logged mAP metrics to TensorBoard:")
+        print(f"  mAP@.50-.95: {map_5095:.4f}")
+        print(f"  mAP@.50: {map_50:.4f}")
+        print(f"  mAP@.75: {map_75:.4f}")
+        
+        # 额外的mAP细分指标
+        if 'mAP_small' in stats:
+            writer.add_scalar('eval/mAP_small', stats['mAP_small'], epoch)
+        if 'mAP_medium' in stats:
+            writer.add_scalar('eval/mAP_medium', stats['mAP_medium'], epoch)
+        if 'mAP_large' in stats:
+            writer.add_scalar('eval/mAP_large', stats['mAP_large'], epoch)
         
-        # Ensure TensorBoard writes are flushed
+        # 强制刷新确保写入
         writer.flush()
+        print("TensorBoard logs flushed successfully")
 
     if coco_evaluator and "segm" in postprocessors.keys():
         stats["coco_eval_masks"] = coco_evaluator.coco_eval["segm"].stats.tolist()

--- End Diff ---

54. 在文件 'rfdetr\engine.py' 中: ### 变更摘要

修改了 `save_detection_images` 函数中获取类别名称的逻辑，主要变更包括：

1. 添加了关于类别ID处理的注释，指出"类别ID从1开始，需要减1"
2. 引入了 `adjusted_category_id` 变量来存储类别ID（尽管实际未执行减1操作）
3. 增加了调试输出，打印类别ID和映射名称，便于排查类别映射问题

此变更似乎是为了解决类别ID与类别名称映射的问题，但注释中提到的减1操作在实际代码中并未实现，可能需要后续完善。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 3bbec3d..feb9f0b 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -514,8 +514,12 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 box = box.cpu().tolist()
                 category_id = label.item()
                 
-                # 获取类别名称
-                category_name = category_mapping.get(category_id, f"Class_{category_id}")
+                # 获取类别名称 - 注意类别ID从1开始，需要减1
+                adjusted_category_id = category_id
+                category_name = category_mapping.get(adjusted_category_id, f"Class_{adjusted_category_id}")
+                
+                # 调试输出
+                print(f"类别ID: {category_id}, 映射名称: {category_name}")
                 
                 # 确保box坐标在图像范围内
                 box = [

--- End Diff ---

55. 在文件 'rfdetr\engine.py' 中: 这个变更修改了`rfdetr/engine.py`文件中的类别映射方式。具体来说，代码将原来的`category_mapping = {i+1: name for i, name in enumerate(COCO_CLASSES)}`修改为直接使用`category_mapping = COCO_CLASSES`。这表明`COCO_CLASSES`可能已经是一个字典格式，不再需要手动创建从索引到类名的映射。变更简化了代码，避免了不必要的映射创建过程。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index feb9f0b..ea07c33 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -487,7 +487,7 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 if not category_mapping:
                     try:
                         from rfdetr.util.coco_classes import COCO_CLASSES
-                        category_mapping = {i+1: name for i, name in enumerate(COCO_CLASSES)}
+                        category_mapping = COCO_CLASSES
                         print(f"Using COCO_CLASSES mapping: {len(category_mapping)} classes")
                     except ImportError:
                         print("COCO_CLASSES not found, using basic mapping")

--- End Diff ---

56. 在文件 'rfdetr\engine.py' 中: # 变更摘要：改进检测图像保存中的类别名称映射逻辑

## 核心变更
修改了 `save_detection_images` 函数中的类别名称获取逻辑，使其更符合实际使用的10类数据集类别映射。

## 具体改动
1. 移除了从 `rfdetr.util.coco_classes` 导入 `COCO_CLASSES` 的尝试，改为直接使用固定的10类数据集映射
2. 更新了类别映射内容，将原来的通用COCO类别替换为更符合实际数据集的类别：
   - 新增：rider, bike, motor, traffic sign
   - 移除：bicycle, motorcycle, airplane, boat
3. 增加了调试信息，打印类别映射的来源和内容
4. 将错误提示信息本地化为中文

## 变更目的
确保检测结果的类别标签与实际使用的数据集类别一致，避免因类别不匹配导致的可视化结果错误。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index ea07c33..224ffd9 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -475,32 +475,37 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
             scaled_pred_boxes = box_ops.box_cxcywh_to_xyxy(pred_boxes)
             scaled_pred_boxes = scaled_pred_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_pred_boxes.device)
             
-            # 改进的类别名称获取逻辑
+            # 改进的类别名称获取逻辑 - 使用实际数据集类别
             category_mapping = {}
             try:
                 # 从coco_dataset获取类别映射
                 if coco_dataset and hasattr(coco_dataset, 'cats'):
                     for cat_id, cat_info in coco_dataset.cats.items():
                         category_mapping[cat_id] = cat_info['name']
+                    print(f"从coco_dataset获取类别: {category_mapping}")
                 
-                # 如果coco_dataset没有类别信息，尝试从util获取
+                # 如果coco_dataset没有类别信息，使用10类数据集的映射
                 if not category_mapping:
-                    try:
-                        from rfdetr.util.coco_classes import COCO_CLASSES
-                        category_mapping = COCO_CLASSES
-                        print(f"Using COCO_CLASSES mapping: {len(category_mapping)} classes")
-                    except ImportError:
-                        print("COCO_CLASSES not found, using basic mapping")
-                        category_mapping = {
-                            1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 5: "airplane",
-                            6: "bus", 7: "train", 8: "truck", 9: "boat", 10: "traffic light"
-                        }
+                    # 10类数据集的映射（与您的数据集匹配）
+                    category_mapping = {
+                        1: "person",
+                        2: "rider",
+                        3: "car",
+                        4: "bus",
+                        5: "truck",
+                        6: "bike",
+                        7: "motor",
+                        8: "traffic light",
+                        9: "traffic sign",
+                        10: "train"
+                    }
+                    print(f"使用10类数据集映射: {category_mapping}")
                 
             except Exception as e:
-                print(f"Error getting category mapping: {e}, using fallback")
+                print(f"获取类别映射时出错: {e}, 使用10类回退方案")
                 category_mapping = {
-                    1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 5: "airplane",
-                    6: "bus", 7: "train", 8: "truck", 9: "boat", 10: "traffic light"
+                    1: "person", 2: "rider", 3: "car", 4: "bus", 5: "truck",
+                    6: "bike", 7: "motor", 8: "traffic light", 9: "traffic sign", 10: "train"
                 }
             
             # 过滤低置信度预测

--- End Diff ---

57. 在文件 'rfdetr\engine.py' 中: # 变更摘要：改进检测图像保存中的类别映射逻辑

本次变更主要针对 `rfdetr/engine.py` 文件中的 `save_detection_images` 函数进行了以下改进：

1. **引入COCO_CLASSES常量**：新增了 `from rfdetr.util.coco_classes import COCO_CLASSES` 导入，使用标准化的COCO类别定义替代硬编码的类别映射。

2. **简化类别映射逻辑**：移除了原有的硬编码10类数据集映射（包括person、rider、car等），改为统一使用COCO_CLASSES作为回退方案，提高了代码的可维护性和一致性。

3. **优化类别ID处理**：删除了不必要的类别ID调整步骤，明确COCO类别ID从1开始且不需要额外调整的逻辑，使代码更加清晰。

4. **改进日志输出**：更新了打印信息，显示使用COCO_CLASSES映射的类别数量，便于调试和问题追踪。

这些变更使类别映射处理更加标准化和健壮，减少了硬编码依赖，提高了代码的可维护性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 224ffd9..c26e995 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -41,6 +41,7 @@ import os
 from PIL import Image, ImageDraw, ImageFont
 import torchvision.transforms.functional as F
 from rfdetr.util import box_ops
+from rfdetr.util.coco_classes import COCO_CLASSES
 import numpy as np
 
 def get_autocast_args(args):
@@ -475,7 +476,7 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
             scaled_pred_boxes = box_ops.box_cxcywh_to_xyxy(pred_boxes)
             scaled_pred_boxes = scaled_pred_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_pred_boxes.device)
             
-            # 改进的类别名称获取逻辑 - 使用实际数据集类别
+            # 改进的类别名称获取逻辑 - 使用COCO_CLASSES
             category_mapping = {}
             try:
                 # 从coco_dataset获取类别映射
@@ -484,29 +485,14 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                         category_mapping[cat_id] = cat_info['name']
                     print(f"从coco_dataset获取类别: {category_mapping}")
                 
-                # 如果coco_dataset没有类别信息，使用10类数据集的映射
+                # 如果coco_dataset没有类别信息，使用COCO_CLASSES
                 if not category_mapping:
-                    # 10类数据集的映射（与您的数据集匹配）
-                    category_mapping = {
-                        1: "person",
-                        2: "rider",
-                        3: "car",
-                        4: "bus",
-                        5: "truck",
-                        6: "bike",
-                        7: "motor",
-                        8: "traffic light",
-                        9: "traffic sign",
-                        10: "train"
-                    }
-                    print(f"使用10类数据集映射: {category_mapping}")
+                    category_mapping = COCO_CLASSES
+                    print(f"使用COCO_CLASSES映射: {len(category_mapping)}个类别")
                 
             except Exception as e:
-                print(f"获取类别映射时出错: {e}, 使用10类回退方案")
-                category_mapping = {
-                    1: "person", 2: "rider", 3: "car", 4: "bus", 5: "truck",
-                    6: "bike", 7: "motor", 8: "traffic light", 9: "traffic sign", 10: "train"
-                }
+                print(f"获取类别映射时出错: {e}, 使用COCO_CLASSES回退方案")
+                category_mapping = COCO_CLASSES
             
             # 过滤低置信度预测
             confidence_threshold = 0.3
@@ -519,9 +505,8 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 box = box.cpu().tolist()
                 category_id = label.item()
                 
-                # 获取类别名称 - 注意类别ID从1开始，需要减1
-                adjusted_category_id = category_id
-                category_name = category_mapping.get(adjusted_category_id, f"Class_{adjusted_category_id}")
+                # 获取类别名称 - COCO类别ID从1开始，不需要调整
+                category_name = category_mapping.get(category_id, f"Class_{category_id}")
                 
                 # 调试输出
                 print(f"类别ID: {category_id}, 映射名称: {category_name}")

--- End Diff ---

58. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

修改了`save_detection_images`函数中的类别ID处理逻辑，解决了模型输出类别ID与COCO类别映射不匹配的问题。

### 主要变更：
1. 增加了对背景类别(ID=0)的处理，直接跳过不显示
2. 添加了类别ID调整逻辑：当类别ID不在映射中但ID-1存在时，自动将ID减1以匹配正确的类别
3. 增加了调试输出，显示类别ID调整过程

### 变更原因：
模型输出包含背景类别(0)且可能使用了num_classes+1的偏移方式，导致类别ID与COCO类别映射不一致。此变更确保了类别名称的正确映射和显示。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index c26e995..6c33fe3 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -505,8 +505,17 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 box = box.cpu().tolist()
                 category_id = label.item()
                 
-                # 获取类别名称 - COCO类别ID从1开始，不需要调整
-                category_name = category_mapping.get(category_id, f"Class_{category_id}")
+                # 获取类别名称 - 模型输出包含背景类别(0)，但COCO_CLASSES从1开始
+                # 如果类别ID为0，表示背景，不显示
+                # 如果类别ID大于COCO_CLASSES的最大ID，尝试减去1（因为模型可能使用了num_classes+1）
+                if category_id == 0:
+                    continue  # 跳过背景类别
+                elif category_id not in category_mapping and category_id - 1 in category_mapping:
+                    adjusted_category_id = category_id - 1
+                    category_name = category_mapping.get(adjusted_category_id, f"Class_{adjusted_category_id}")
+                    print(f"调整类别ID: {category_id} -> {adjusted_category_id}, 类别名称: {category_name}")
+                else:
+                    category_name = category_mapping.get(category_id, f"Class_{category_id}")
                 
                 # 调试输出
                 print(f"类别ID: {category_id}, 映射名称: {category_name}")

--- End Diff ---

59. 在文件 'result\782e5511-8eaa78ed\epoch_0.png' 中: ## 变更摘要

删除了位于`result/782e5511-8eaa78ed/`目录下的`epoch_0.png`图片文件。该文件是二进制图像文件，从文件名推测可能是机器学习模型训练过程中第0个epoch的可视化结果。此次变更将该文件从代码库中完全移除。
--- Git Diff ---
diff --git a/result/782e5511-8eaa78ed/epoch_0.png b/result/782e5511-8eaa78ed/epoch_0.png
deleted file mode 100644
index 3f1438c..0000000
Binary files a/result/782e5511-8eaa78ed/epoch_0.png and /dev/null differ

--- End Diff ---

60. 在文件 'result\26089727-91d1e3f2\epoch_0.png' 中: 删除了训练结果目录中的epoch_0.png图像文件（位于result/26089727-91d1e3f2/目录下）。该文件是机器学习训练过程中第0个epoch的可视化结果。
--- Git Diff ---
diff --git a/result/26089727-91d1e3f2/epoch_0.png b/result/26089727-91d1e3f2/epoch_0.png
deleted file mode 100644
index 9040022..0000000
Binary files a/result/26089727-91d1e3f2/epoch_0.png and /dev/null differ

--- End Diff ---

61. 在文件 'result\27af350c-59cf32cc\epoch_0.png' 中: 删除了位于result/27af350c-59cf32cc/目录下的epoch_0.png图片文件。该文件可能是某个模型训练过程中的第0轮(epoch)结果可视化图像。
--- Git Diff ---
diff --git a/result/27af350c-59cf32cc/epoch_0.png b/result/27af350c-59cf32cc/epoch_0.png
deleted file mode 100644
index 1b2c925..0000000
Binary files a/result/27af350c-59cf32cc/epoch_0.png and /dev/null differ

--- End Diff ---





62. 在文件 'rfdetr\engine.py' 中: # 变更摘要：优化检测图像可视化功能

本次变更主要优化了`save_detection_images`函数的检测可视化逻辑，核心修改包括：

1. **降低置信度阈值**：从0.3降至0.1，使更多预测框能够显示在可视化结果中，特别有助于推理阶段的可视化分析。

2. **区分高低置信度预测**：
   - 不再直接过滤低置信度预测，而是保留并标记
   - 对低置信度(<0.3)预测框使用橙色(高置信度使用红色)
   - 低置信度预测框线宽较细(2px vs 3px)
   - 在标签文本中添加"(low)"标记

3. **增强调试信息**：在打印输出中增加置信度值显示，便于调试分析。

4. **改进文本显示**：为低置信度预测使用不同的文本颜色方案，提高可视化区分度。

这些变更使得检测结果可视化更加全面和直观，便于开发者分析模型性能，特别是对低置信度预测的观察。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 6c33fe3..78dffd7 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -495,13 +495,14 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 category_mapping = COCO_CLASSES
             
             # 过滤低置信度预测
-            confidence_threshold = 0.3
+            # 降低阈值以便显示更多预测框，特别是对于推理可视化
+            confidence_threshold = 0.1
             
             for box, label, score in zip(scaled_pred_boxes, labels, scores):
                 score_value = score.item()
-                if score_value < confidence_threshold:
-                    continue
-                    
+                # 即使置信度较低也显示，但可以添加一个标记
+                is_low_confidence = score_value < 0.3
+                
                 box = box.cpu().tolist()
                 category_id = label.item()
                 
@@ -518,7 +519,7 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                     category_name = category_mapping.get(category_id, f"Class_{category_id}")
                 
                 # 调试输出
-                print(f"类别ID: {category_id}, 映射名称: {category_name}")
+                print(f"类别ID: {category_id}, 映射名称: {category_name}, 置信度: {score_value:.3f}")
                 
                 # 确保box坐标在图像范围内
                 box = [
@@ -528,11 +529,16 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                     max(0, min(box[3], img_h))
                 ]
                 
+                # 对于低置信度的预测框，使用不同的颜色
+                box_color = "orange" if is_low_confidence else "red"
+                box_width = 2 if is_low_confidence else 3
+                
                 # 绘制边界框
-                draw.rectangle(box, outline="red", width=3)
+                draw.rectangle(box, outline=box_color, width=box_width)
                 
                 # 准备文本
-                text = f"{category_name}: {score_value:.2f}"
+                confidence_text = f"{score_value:.2f}" + (" (low)" if is_low_confidence else "")
+                text = f"{category_name}: {confidence_text}"
                 
                 # 计算文本位置 - 确保在图像内
                 text_x = max(2, min(box[0] + 2, img_w - 100))
@@ -544,6 +550,10 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 text_position = (text_x, text_y)
                 
+                # 对于低置信度的预测框，使用不同的文本颜色
+                text_color = "orange" if is_low_confidence else "white"
+                bg_color = "red"  # 保持背景色一致
+                
                 # 绘制带背景的文本
                 try:
                     # 获取文本边界框
@@ -574,17 +584,17 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                     bg_box[3] = min(img_h, bg_box[3])
                     
                     # 绘制背景
-                    draw.rectangle(bg_box, fill="red")
+                    draw.rectangle(bg_box, fill=bg_color)
                     
                     # 绘制文本
-                    draw.text(text_position, text, fill="white", font=font)
+                    draw.text(text_position, text, fill=text_color, font=font)
                     drawn_boxes += 1
                     
                 except Exception as e:
                     print(f"Error drawing text for {category_name}: {e}")
                     # 最后的备选：直接绘制文本
                     try:
-                        draw.text(text_position, text, fill="red", font=font)
+                        draw.text(text_position, text, fill=box_color, font=font)
                         drawn_boxes += 1
                     except:
                         pass

--- End Diff ---

63. 在文件 'rfdetr\models\lwdetr.py' 中: ## 变更摘要

修改了LWDETR模型中关于参考点嵌入和查询特征权重的使用逻辑。主要变更内容：

1. **调整条件判断**：将原来的`if self.training`条件扩展为`if self.training or not self.two_stage`，使非two_stage模型在推理时也能使用全部权重。

2. **优化推理策略**：非two_stage模型现在在推理时使用所有group的权重，以提高检测效果；而two_stage模型在推理时仍保持原有策略，仅使用部分权重（前self.num_queries个）。

3. **增加注释说明**：添加了中文注释，明确解释了变更目的——在推理时使用所有group以提高检测效果，同时保持评估时的一致性。

此变更旨在提升非two_stage模型在推理阶段的检测性能，同时保持two_stage模型的原有行为不变。
--- Git Diff ---
diff --git a/rfdetr/models/lwdetr.py b/rfdetr/models/lwdetr.py
index 00c6e2f..8f330e5 100644
--- a/rfdetr/models/lwdetr.py
+++ b/rfdetr/models/lwdetr.py
@@ -153,11 +153,13 @@ class LWDETR(nn.Module):
             masks.append(mask)
             assert mask is not None
 
-        if self.training:
+        # 在推理时也使用所有group，以提高检测效果
+        # 但在评估时仍然只使用一个group以保持一致性
+        if self.training or not self.two_stage:
             refpoint_embed_weight = self.refpoint_embed.weight
             query_feat_weight = self.query_feat.weight
         else:
-            # only use one group in inference
+            # only use one group in inference for two_stage models
             refpoint_embed_weight = self.refpoint_embed.weight[:self.num_queries]
             query_feat_weight = self.query_feat.weight[:self.num_queries]
 

--- End Diff ---

64. 在文件 'rfdetr\models\lwdetr.py' 中: 本次变更修改了 RFDetr 模型中的后处理模块 (PostProcess)，主要调整了预测框的选择策略。具体变更包括：

1. 增加了选择的预测框数量，从原来的 `self.num_select` 调整为 `min(self.num_select * 2, out_logits.shape[1])`，即增加一倍但不超过总查询数。
2. 添加了注释说明变更目的是为了在可视化时能够显示更多结果。

这一变更使得模型在推理阶段会选择更多的预测框进行后续处理，有助于在可视化阶段提供更丰富的检测结果展示。
--- Git Diff ---
diff --git a/rfdetr/models/lwdetr.py b/rfdetr/models/lwdetr.py
index 8f330e5..e3659b6 100644
--- a/rfdetr/models/lwdetr.py
+++ b/rfdetr/models/lwdetr.py
@@ -553,7 +553,10 @@ class PostProcess(nn.Module):
         assert target_sizes.shape[1] == 2
 
         prob = out_logits.sigmoid()
-        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), self.num_select, dim=1)
+        
+        # 增加选择的预测框数量，以便在可视化时显示更多结果
+        num_select = min(self.num_select * 2, out_logits.shape[1])  # 增加一倍但不超过总查询数
+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), num_select, dim=1)
         scores = topk_values
         topk_boxes = topk_indexes // out_logits.shape[2]
         labels = topk_indexes % out_logits.shape[2]

--- End Diff ---

65. 在文件 'rfdetr\cli\main.py' 中: 
--- Git Diff ---
diff --git a/rfdetr/cli/main.py b/rfdetr/cli/main.py
index 45a5581..b5ff57d 100644
--- a/rfdetr/cli/main.py
+++ b/rfdetr/cli/main.py
@@ -22,6 +22,9 @@ import roboflow
 from rfdetr import RFDETRBase
 import torch
 import os
+from PIL import Image
+import torchvision.transforms.functional as F
+import numpy as np
 
 def download_dataset(rf_project: roboflow.Project, dataset_version: int):
     versions = rf_project.versions()

--- End Diff ---

66. 在文件 'rfdetr\engine.py' 中: # 变更摘要：调整目标检测可视化中的置信度阈值和显示样式

本次变更修改了`rfdetr/engine.py`中的`save_detection_images`函数，主要调整了目标检测结果的可视化逻辑：

1. **提高置信度阈值**：将显示预测框的置信度阈值从0.1提高到0.3，减少低质量预测框的显示，提升可视化结果的质量。

2. **统一显示样式**：移除了对低置信度预测框的特殊标记（橙色边框和"low"标记），所有预测框统一使用红色边框和白色文本显示。

3. **增加调试信息**：添加了过滤前后的预测框数量统计，并在没有预测框被绘制时提供可能的原因分析，帮助用户理解可视化结果。

这些变更旨在提供更清晰、更高质量的可视化结果，同时增强调试功能，便于用户分析模型预测行为。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 78dffd7..ad225be 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -495,13 +495,18 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 category_mapping = COCO_CLASSES
             
             # 过滤低置信度预测
-            # 降低阈值以便显示更多预测框，特别是对于推理可视化
-            confidence_threshold = 0.1
+            # 使用更合理的阈值来显示预测框
+            confidence_threshold = 0.3  # 提高阈值以减少低质量预测框
+            
+            # 添加调试信息，帮助用户理解为什么有些图片没有预测框
+            print(f"Total predicted boxes before filtering: {len(scaled_pred_boxes)}")
             
             for box, label, score in zip(scaled_pred_boxes, labels, scores):
                 score_value = score.item()
-                # 即使置信度较低也显示，但可以添加一个标记
-                is_low_confidence = score_value < 0.3
+                
+                # 只显示高于阈值的预测框
+                if score_value < confidence_threshold:
+                    continue
                 
                 box = box.cpu().tolist()
                 category_id = label.item()
@@ -529,16 +534,15 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                     max(0, min(box[3], img_h))
                 ]
                 
-                # 对于低置信度的预测框，使用不同的颜色
-                box_color = "orange" if is_low_confidence else "red"
-                box_width = 2 if is_low_confidence else 3
+                # 使用统一的颜色和宽度绘制边界框
+                box_color = "red"
+                box_width = 3
                 
                 # 绘制边界框
                 draw.rectangle(box, outline=box_color, width=box_width)
                 
                 # 准备文本
-                confidence_text = f"{score_value:.2f}" + (" (low)" if is_low_confidence else "")
-                text = f"{category_name}: {confidence_text}"
+                text = f"{category_name}: {score_value:.2f}"
                 
                 # 计算文本位置 - 确保在图像内
                 text_x = max(2, min(box[0] + 2, img_w - 100))
@@ -550,8 +554,8 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 text_position = (text_x, text_y)
                 
-                # 对于低置信度的预测框，使用不同的文本颜色
-                text_color = "orange" if is_low_confidence else "white"
+                # 使用统一的文本颜色
+                text_color = "white"
                 bg_color = "red"  # 保持背景色一致
                 
                 # 绘制带背景的文本
@@ -598,6 +602,14 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                         drawn_boxes += 1
                     except:
                         pass
+            
+            # 添加最终的调试信息
+            print(f"Total predicted boxes after filtering: {drawn_boxes}")
+            if drawn_boxes == 0:
+                print("警告: 没有预测框被绘制。可能的原因:")
+                print("1. 所有预测的置信度都低于阈值 ({:.2f})".format(confidence_threshold))
+                print("2. 模型可能没有正确训练或加载")
+                print("3. 输入图像可能不包含模型训练时见过的类别")
         else:
             print("No predicted boxes to draw.")
     else:

--- End Diff ---

67. 在文件 'rfdetr\engine.py' 中: ### 变更摘要

修复了`save_detection_images`函数中的类别ID映射逻辑，解决了模型输出类别与COCO类别名称不匹配的问题。

#### 核心变更：
1. **重写类别映射逻辑**：明确区分模型输出的类别索引(0-based)和COCO类别索引(1-based)，首先尝试直接映射，失败时回退到原始ID
2. **改进处理流程**：简化了条件判断结构，使代码更清晰易读
3. **增强调试输出**：添加更详细的日志信息，便于排查类别映射问题
4. **完善注释**：更清晰地解释了类别索引转换的原理和背景

此变更解决了模型输出类别与COCO数据集类别名称之间的不一致问题，特别是在模型使用num_classes+1输出维度的情况下的映射错误。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index ad225be..e406877 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -511,17 +511,25 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 box = box.cpu().tolist()
                 category_id = label.item()
                 
-                # 获取类别名称 - 模型输出包含背景类别(0)，但COCO_CLASSES从1开始
-                # 如果类别ID为0，表示背景，不显示
-                # 如果类别ID大于COCO_CLASSES的最大ID，尝试减去1（因为模型可能使用了num_classes+1）
+                # 获取类别名称 - 修复类别索引问题
+                # 模型输出的类别索引是从0开始的，其中0通常表示背景类别
+                # COCO_CLASSES使用的是1-based索引（1-90），没有0类别
+                # 因此我们需要将模型输出的索引+1来匹配COCO_CLASSES的索引
                 if category_id == 0:
                     continue  # 跳过背景类别
-                elif category_id not in category_mapping and category_id - 1 in category_mapping:
-                    adjusted_category_id = category_id - 1
-                    category_name = category_mapping.get(adjusted_category_id, f"Class_{adjusted_category_id}")
-                    print(f"调整类别ID: {category_id} -> {adjusted_category_id}, 类别名称: {category_name}")
-                else:
+                
+                # 将模型输出的类别索引转换为COCO类别索引（+1）
+                coco_category_id = category_id
+                
+                # 如果转换后的ID在category_mapping中不存在，尝试使用原始ID
+                if coco_category_id not in category_mapping:
+                    # 这可能是因为模型使用了num_classes+1的输出维度
+                    # 在这种情况下，类别ID应该与模型训练时的类别ID对应
                     category_name = category_mapping.get(category_id, f"Class_{category_id}")
+                    print(f"使用原始类别ID {category_id} 映射到类别名称: {category_name}")
+                else:
+                    category_name = category_mapping.get(coco_category_id, f"Class_{coco_category_id}")
+                    print(f"使用COCO类别ID {coco_category_id} 映射到类别名称: {category_name}")
                 
                 # 调试输出
                 print(f"类别ID: {category_id}, 映射名称: {category_name}, 置信度: {score_value:.3f}")

--- End Diff ---

68. 在文件 'rfdetr\engine.py' 中: 变更摘要：修改了 `save_detection_images` 函数中的类别ID映射逻辑。主要变更包括：1) 显式地将模型输出的类别ID加1转换为COCO类别索引（之前未执行此加1操作）；2) 移除了条件判断逻辑，不再检查映射后的ID是否存在于category_mapping中；3) 简化了类别名称获取流程，统一使用转换后的ID进行映射。此变更简化了类别ID到COCO类别的映射处理，确保始终使用正确的COCO类别索引格式。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index e406877..069d1c7 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -519,17 +519,11 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                     continue  # 跳过背景类别
                 
                 # 将模型输出的类别索引转换为COCO类别索引（+1）
-                coco_category_id = category_id
+                coco_category_id = category_id + 1
                 
-                # 如果转换后的ID在category_mapping中不存在，尝试使用原始ID
-                if coco_category_id not in category_mapping:
-                    # 这可能是因为模型使用了num_classes+1的输出维度
-                    # 在这种情况下，类别ID应该与模型训练时的类别ID对应
-                    category_name = category_mapping.get(category_id, f"Class_{category_id}")
-                    print(f"使用原始类别ID {category_id} 映射到类别名称: {category_name}")
-                else:
-                    category_name = category_mapping.get(coco_category_id, f"Class_{coco_category_id}")
-                    print(f"使用COCO类别ID {coco_category_id} 映射到类别名称: {category_name}")
+                # 使用转换后的ID获取类别名称
+                category_name = category_mapping.get(coco_category_id, f"Class_{coco_category_id}")
+                print(f"使用COCO类别ID {coco_category_id} 映射到类别名称: {category_name}")
                 
                 # 调试输出
                 print(f"类别ID: {category_id}, 映射名称: {category_name}, 置信度: {score_value:.3f}")

--- End Diff ---

69. 在文件 'rfdetr\config.py' 中: 本次变更向 `TrainConfig` 类中添加了一个新的可选参数 `train_annotations_path`，类型为 `Optional[str]`，默认值为 `None`。这个参数允许用户自定义训练标注文件的路径，提供了更灵活的数据集配置方式。由于该参数是可选的且具有默认值，因此这是一个向后兼容的变更，不会影响现有代码的功能。
--- Git Diff ---
diff --git a/rfdetr/config.py b/rfdetr/config.py
index 8dda823..c5be9f9 100644
--- a/rfdetr/config.py
+++ b/rfdetr/config.py
@@ -73,6 +73,7 @@ class TrainConfig(BaseModel):
     dataset_file: Literal["coco", "o365", "roboflow"] = "coco"
     square_resize_div_64: bool = True
     dataset_dir: str = "./dataset"
+    train_annotations_path: Optional[str] = None
     output_dir: str = "output"
     multi_scale: bool = True
     expanded_scales: bool = True

--- End Diff ---

70. 在文件 'rfdetr\detr.py' 中: **变更摘要：**

在 `rfdetr/detr.py` 文件的 `train_from_config` 方法中，改进了训练标注文件路径的处理方式。变更前使用硬编码路径，变更后增加了灵活性，允许通过 `TrainConfig` 中的 `train_annotations_path` 参数自定义标注文件路径。当未提供该参数时，系统会回退到默认路径并记录警告日志，建议用户明确设置此值以提高配置的清晰度。此变更保持了向后兼容性，同时增强了配置的灵活性。
--- Git Diff ---
diff --git a/rfdetr/detr.py b/rfdetr/detr.py
index 5809e25..45591b9 100644
--- a/rfdetr/detr.py
+++ b/rfdetr/detr.py
@@ -94,9 +94,15 @@ class RFDETR:
         self.model.export(**kwargs)
 
     def train_from_config(self, config: TrainConfig, **kwargs):
-        with open(
-            os.path.join(config.dataset_dir, "train", "_annotations.coco.json"), "r"
-        ) as f:
+        annotations_path = config.train_annotations_path
+        if annotations_path is None:
+            annotations_path = os.path.join(config.dataset_dir, "train", "_annotations.coco.json")
+            logger.warning(
+                f"train_annotations_path not provided, defaulting to {annotations_path}. "
+                f"It is recommended to set this value explicitly in your TrainConfig."
+            )
+        
+        with open(annotations_path, "r") as f:
             anns = json.load(f)
             num_classes = len(anns["categories"])
             class_names = [c["name"] for c in anns["categories"] if c["supercategory"] != "none"]

--- End Diff ---

71. 在文件 'rfdetr\detr.py' 中: 本次变更主要简化了 `train_from_config` 方法中关于训练注释文件路径的处理逻辑。具体变更包括：

1. 移除了条件判断和警告日志，不再检查 `train_annotations_path` 是否为 None
2. 直接使用固定的路径 `os.path.join(config.dataset_dir, "train", "_annotations.coco.json")` 来加载注释文件
3. 删除了相关的警告日志，该日志原本在未提供 `train_annotations_path` 时会提示用户使用默认路径

这一变更简化了代码逻辑，但降低了灵活性，不再允许用户通过配置指定自定义的训练注释文件路径。
--- Git Diff ---
diff --git a/rfdetr/detr.py b/rfdetr/detr.py
index 45591b9..5809e25 100644
--- a/rfdetr/detr.py
+++ b/rfdetr/detr.py
@@ -94,15 +94,9 @@ class RFDETR:
         self.model.export(**kwargs)
 
     def train_from_config(self, config: TrainConfig, **kwargs):
-        annotations_path = config.train_annotations_path
-        if annotations_path is None:
-            annotations_path = os.path.join(config.dataset_dir, "train", "_annotations.coco.json")
-            logger.warning(
-                f"train_annotations_path not provided, defaulting to {annotations_path}. "
-                f"It is recommended to set this value explicitly in your TrainConfig."
-            )
-        
-        with open(annotations_path, "r") as f:
+        with open(
+            os.path.join(config.dataset_dir, "train", "_annotations.coco.json"), "r"
+        ) as f:
             anns = json.load(f)
             num_classes = len(anns["categories"])
             class_names = [c["name"] for c in anns["categories"] if c["supercategory"] != "none"]

--- End Diff ---

72. 在文件 'rfdetr\detr.py' 中: ### 变更摘要

修改了`RFDETR`类中的`train_from_config`方法，增加了训练注释文件路径的灵活性。变更前直接使用硬编码路径，现在改为优先从`config.train_annotations_path`获取路径配置，若未配置则回退到默认路径，并添加警告日志建议用户显式设置该值。这一改进提高了配置灵活性，同时通过警告提示用户最佳实践。
--- Git Diff ---
diff --git a/rfdetr/detr.py b/rfdetr/detr.py
index 5809e25..45591b9 100644
--- a/rfdetr/detr.py
+++ b/rfdetr/detr.py
@@ -94,9 +94,15 @@ class RFDETR:
         self.model.export(**kwargs)
 
     def train_from_config(self, config: TrainConfig, **kwargs):
-        with open(
-            os.path.join(config.dataset_dir, "train", "_annotations.coco.json"), "r"
-        ) as f:
+        annotations_path = config.train_annotations_path
+        if annotations_path is None:
+            annotations_path = os.path.join(config.dataset_dir, "train", "_annotations.coco.json")
+            logger.warning(
+                f"train_annotations_path not provided, defaulting to {annotations_path}. "
+                f"It is recommended to set this value explicitly in your TrainConfig."
+            )
+        
+        with open(annotations_path, "r") as f:
             anns = json.load(f)
             num_classes = len(anns["categories"])
             class_names = [c["name"] for c in anns["categories"] if c["supercategory"] != "none"]

--- End Diff ---

73. 在文件 'rfdetr\main.py' 中: ## 变更摘要

在`rfdetr/main.py`中添加了新的训练标注路径配置选项：

1. 在`populate_args`函数中新增`train_annotations_path`参数，默认值为None
2. 在参数解析器`get_args_parser`中添加了对应的命令行参数`--train_annotations_path`
3. 该参数允许用户直接指定训练标注文件的路径，覆盖程序默认的路径构建方式

这次变更提供了更灵活的数据集配置方式，使用户能够自定义训练标注文件的位置，而不必遵循默认的路径结构。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 255da4b..2852736 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -645,6 +645,7 @@ def populate_args(
     dataset_file='coco',
     coco_path=None,
     dataset_dir=None,
+    train_annotations_path=None,
     square_resize_div_64=False,
     
     # Output parameters
@@ -753,6 +754,7 @@ def populate_args(
         dataset_file=dataset_file,
         coco_path=coco_path,
         dataset_dir=dataset_dir,
+        train_annotations_path=train_annotations_path,
         square_resize_div_64=square_resize_div_64,
         output_dir=output_dir,
         dont_save_weights=dont_save_weights,
@@ -899,6 +901,8 @@ def get_args_parser():
     parser.add_argument('--dataset_file', default='coco')
     parser.add_argument('--coco_path', type=str)
     parser.add_argument('--dataset_dir', type=str, default='./dataset')
+    parser.add_argument('--train_annotations_path', type=str, default=None,
+                        help='Path to the training annotations file. Overrides the default path construction.')
     parser.add_argument('--square_resize_div_64', action='store_true')
 
     parser.add_argument('--output_dir', default='output',

--- End Diff ---

74. 在文件 'rfdetr\config.py' 中: 本次代码变更修改了 `rfdetr/config.py` 文件中的 `TrainConfig` 类，具体变更内容如下：

将 `train_annotations_path` 参数的默认值从 `None` 修改为 `"rf-detr/dataset/annotations/instances_train2017.json"`。这意味着当用户未显式指定训练标注文件路径时，系统将默认使用 COCO 数据集的 2017 年训练实例标注文件路径。

这个变更简化了配置流程，使用户在使用 COCO 数据集时无需手动指定训练标注文件的路径，提高了用户体验。
--- Git Diff ---
diff --git a/rfdetr/config.py b/rfdetr/config.py
index c5be9f9..5e856fd 100644
--- a/rfdetr/config.py
+++ b/rfdetr/config.py
@@ -73,7 +73,7 @@ class TrainConfig(BaseModel):
     dataset_file: Literal["coco", "o365", "roboflow"] = "coco"
     square_resize_div_64: bool = True
     dataset_dir: str = "./dataset"
-    train_annotations_path: Optional[str] = None
+    train_annotations_path: Optional[str] = "rf-detr/dataset/annotations/instances_train2017.json"
     output_dir: str = "output"
     multi_scale: bool = True
     expanded_scales: bool = True

--- End Diff ---

75. 在文件 'rfdetr\main.py' 中: 变更摘要：修改了rfdetr/main.py中get_args_parser()函数的--train_annotations_path参数，将其默认值从None改为'rf-detr/dataset/annotations/instances_train2017.json'，并简化了帮助文本。此变更确保了在不显式指定训练注释路径时，程序将使用COCO数据集的标准训练注释文件作为默认值。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 2852736..4cbd3a4 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -901,8 +901,8 @@ def get_args_parser():
     parser.add_argument('--dataset_file', default='coco')
     parser.add_argument('--coco_path', type=str)
     parser.add_argument('--dataset_dir', type=str, default='./dataset')
-    parser.add_argument('--train_annotations_path', type=str, default=None,
-                        help='Path to the training annotations file. Overrides the default path construction.')
+    parser.add_argument('--train_annotations_path', type=str, default='rf-detr/dataset/annotations/instances_train2017.json',
+                        help='Path to the training annotations file.')
     parser.add_argument('--square_resize_div_64', action='store_true')
 
     parser.add_argument('--output_dir', default='output',

--- End Diff ---

76. 在文件 'rfdetr\detr.py' 中: 本次变更主要简化了RFDETR类中train_from_config方法的代码逻辑。具体变更如下：

1. 移除了对train_annotations_path参数的默认值处理逻辑，包括：
   - 不再在config.train_annotations_path为None时提供默认路径
   - 删除了相关的警告日志记录

2. 直接使用config.train_annotations_path作为注释文件路径，不再进行额外的路径检查和默认值设置

3. 保留了从注释文件中加载类别信息的功能，包括获取类别数量和名称

这次变更使代码更加简洁，但同时也要求用户必须在TrainConfig中明确提供train_annotations_path参数，否则会导致错误。
--- Git Diff ---
diff --git a/rfdetr/detr.py b/rfdetr/detr.py
index 45591b9..8a82375 100644
--- a/rfdetr/detr.py
+++ b/rfdetr/detr.py
@@ -94,15 +94,7 @@ class RFDETR:
         self.model.export(**kwargs)
 
     def train_from_config(self, config: TrainConfig, **kwargs):
-        annotations_path = config.train_annotations_path
-        if annotations_path is None:
-            annotations_path = os.path.join(config.dataset_dir, "train", "_annotations.coco.json")
-            logger.warning(
-                f"train_annotations_path not provided, defaulting to {annotations_path}. "
-                f"It is recommended to set this value explicitly in your TrainConfig."
-            )
-        
-        with open(annotations_path, "r") as f:
+        with open(config.train_annotations_path, "r") as f:
             anns = json.load(f)
             num_classes = len(anns["categories"])
             class_names = [c["name"] for c in anns["categories"] if c["supercategory"] != "none"]

--- End Diff ---

77. 在文件 'rfdetr\config.py' 中: ## 变更摘要

在`rfdetr/config.py`文件的`TrainConfig`类中添加了一个新的配置参数`val_annotations_path`，用于指定验证集标注文件的路径。该参数为可选类型（Optional[str]），默认值为None。此变更与已有的`train_annotations_path`参数相对应，完善了训练配置中对验证集的支持，使得模型在训练过程中可以加载验证集数据进行评估。
--- Git Diff ---
diff --git a/rfdetr/config.py b/rfdetr/config.py
index 5e856fd..caac03a 100644
--- a/rfdetr/config.py
+++ b/rfdetr/config.py
@@ -74,6 +74,7 @@ class TrainConfig(BaseModel):
     square_resize_div_64: bool = True
     dataset_dir: str = "./dataset"
     train_annotations_path: Optional[str] = "rf-detr/dataset/annotations/instances_train2017.json"
+    val_annotations_path: Optional[str] = None
     output_dir: str = "output"
     multi_scale: bool = True
     expanded_scales: bool = True

--- End Diff ---

78. 在文件 'rfdetr\main.py' 中: **变更摘要**：

在 `rfdetr/main.py` 中添加了对验证集注释文件路径的自定义配置支持。具体变更包括：

1. 在 `populate_args()` 函数中新增了 `val_annotations_path` 参数，允许传递验证集注释文件路径
2. 在参数解析器 `get_args_parser()` 中添加了 `--val_annotations_path` 命令行选项，默认值为 None
3. 该参数可覆盖默认的验证集注释文件路径构建方式，提供了更灵活的数据集配置能力

此变更使用户能够显式指定验证集注释文件的位置，增强了模型对不同数据集组织结构的适应性。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 4cbd3a4..d6ae981 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -646,6 +646,7 @@ def populate_args(
     coco_path=None,
     dataset_dir=None,
     train_annotations_path=None,
+    val_annotations_path=None,
     square_resize_div_64=False,
     
     # Output parameters
@@ -755,6 +756,7 @@ def populate_args(
         coco_path=coco_path,
         dataset_dir=dataset_dir,
         train_annotations_path=train_annotations_path,
+        val_annotations_path=val_annotations_path,
         square_resize_div_64=square_resize_div_64,
         output_dir=output_dir,
         dont_save_weights=dont_save_weights,
@@ -903,6 +905,8 @@ def get_args_parser():
     parser.add_argument('--dataset_dir', type=str, default='./dataset')
     parser.add_argument('--train_annotations_path', type=str, default='rf-detr/dataset/annotations/instances_train2017.json',
                         help='Path to the training annotations file.')
+    parser.add_argument('--val_annotations_path', type=str, default=None,
+                        help='Path to the validation annotations file. Overrides the default path construction.')
     parser.add_argument('--square_resize_div_64', action='store_true')
 
     parser.add_argument('--output_dir', default='output',

--- End Diff ---

79. 在文件 'rfdetr\config.py' 中: 本次变更修改了 `rfdetr/config.py` 文件中的 `TrainConfig` 类，将 `val_annotations_path` 参数的默认值从 `None` 修改为 `"rf-detr/dataset/annotations/instances_val2017.json"`。这个变更确保了验证阶段有默认的验证注释文件路径，使得模型训练时能够自动使用验证数据集进行评估，而无需手动指定验证注释路径。
--- Git Diff ---
diff --git a/rfdetr/config.py b/rfdetr/config.py
index caac03a..a2f08c9 100644
--- a/rfdetr/config.py
+++ b/rfdetr/config.py
@@ -74,7 +74,7 @@ class TrainConfig(BaseModel):
     square_resize_div_64: bool = True
     dataset_dir: str = "./dataset"
     train_annotations_path: Optional[str] = "rf-detr/dataset/annotations/instances_train2017.json"
-    val_annotations_path: Optional[str] = None
+    val_annotations_path: Optional[str] = "rf-detr/dataset/annotations/instances_val2017.json"
     output_dir: str = "output"
     multi_scale: bool = True
     expanded_scales: bool = True

--- End Diff ---

80. 在文件 'rfdetr\main.py' 中: ## 变更摘要

修改了`rfdetr/main.py`中的命令行参数配置，将`--val_annotations_path`参数的默认值从`None`更改为具体的默认路径`'rf-detr/dataset/annotations/instances_val2017.json'`，同时简化了该参数的帮助文本。这一变更使用户在不指定验证集注释路径时，能够自动使用预设的验证集注释文件，提高了工具的易用性。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index d6ae981..e06d660 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -905,8 +905,8 @@ def get_args_parser():
     parser.add_argument('--dataset_dir', type=str, default='./dataset')
     parser.add_argument('--train_annotations_path', type=str, default='rf-detr/dataset/annotations/instances_train2017.json',
                         help='Path to the training annotations file.')
-    parser.add_argument('--val_annotations_path', type=str, default=None,
-                        help='Path to the validation annotations file. Overrides the default path construction.')
+    parser.add_argument('--val_annotations_path', type=str, default='rf-detr/dataset/annotations/instances_val2017.json',
+                        help='Path to the validation annotations file.')
     parser.add_argument('--square_resize_div_64', action='store_true')
 
     parser.add_argument('--output_dir', default='output',

--- End Diff ---

81. 在文件 'rfdetr\detr.py' 中: 本次变更对`rfdetr/detr.py`文件中的`train_from_config`方法进行了修改。变更内容包括：

1. 添加了两行注释，说明相关逻辑现在由数据集构建器和主训练循环处理。
2. 保留了对训练注释文件的读取和类别数量计算，因为需要将类别数量传递给训练循环。

这次变更的核心是将部分训练逻辑从该方法中移出，改为由数据集构建器和主训练循环处理，但仍保留了计算类别数量的功能，因为该信息仍需传递给训练循环。
--- Git Diff ---
diff --git a/rfdetr/detr.py b/rfdetr/detr.py
index 8a82375..d26fe89 100644
--- a/rfdetr/detr.py
+++ b/rfdetr/detr.py
@@ -94,6 +94,8 @@ class RFDETR:
         self.model.export(**kwargs)
 
     def train_from_config(self, config: TrainConfig, **kwargs):
+        # This logic is now handled by the dataset builder and the main training loop
+        # We still need to pass the num_classes to the training loop
         with open(config.train_annotations_path, "r") as f:
             anns = json.load(f)
             num_classes = len(anns["categories"])

--- End Diff ---

82. 在文件 'rfdetr\config.py' 中: ## 变更摘要

修改了 `rfdetr/config.py` 文件中 `TrainConfig` 类的默认路径配置：

- 将 `train_annotations_path` 的默认值从 `"rf-detr/dataset/annotations/instances_train2017.json"` 更改为 `"./dataset/annotations/instances_train2017.json"`
- 将 `val_annotations_path` 的默认值从 `"rf-detr/dataset/annotations/instances_val2017.json"` 更改为 `"./dataset/annotations/instances_val2017.json"`

此变更简化了数据集注释文件的默认路径，使其与配置中已有的 `dataset_dir` 路径格式保持一致，提高了配置文件的一致性和可维护性。
--- Git Diff ---
diff --git a/rfdetr/config.py b/rfdetr/config.py
index a2f08c9..f71cdc0 100644
--- a/rfdetr/config.py
+++ b/rfdetr/config.py
@@ -73,8 +73,8 @@ class TrainConfig(BaseModel):
     dataset_file: Literal["coco", "o365", "roboflow"] = "coco"
     square_resize_div_64: bool = True
     dataset_dir: str = "./dataset"
-    train_annotations_path: Optional[str] = "rf-detr/dataset/annotations/instances_train2017.json"
-    val_annotations_path: Optional[str] = "rf-detr/dataset/annotations/instances_val2017.json"
+    train_annotations_path: Optional[str] = "./dataset/annotations/instances_train2017.json"
+    val_annotations_path: Optional[str] = "./dataset/annotations/instances_val2017.json"
     output_dir: str = "output"
     multi_scale: bool = True
     expanded_scales: bool = True

--- End Diff ---

83. 在文件 'rfdetr\main.py' 中: ## 变更摘要

修改了RF-DETR主程序中训练和验证数据集注释文件的默认路径参数，将路径前缀从`'rf-detr/'`更改为`'./'`。具体变更包括：

1. `--train_annotations_path`默认值从`'rf-detr/dataset/annotations/instances_train2017.json'`改为`'./dataset/annotations/instances_train2017.json'`
2. `--val_annotations_path`默认值从`'rf-detr/dataset/annotations/instances_val2017.json'`改为`'./dataset/annotations/instances_val2017.json'`

此变更调整了数据集注释文件的相对路径引用方式，使其直接从当前工作目录而非项目子目录引用，可能反映了项目目录结构的调整或路径引用方式的规范化。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index e06d660..2cc77d7 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -903,9 +903,9 @@ def get_args_parser():
     parser.add_argument('--dataset_file', default='coco')
     parser.add_argument('--coco_path', type=str)
     parser.add_argument('--dataset_dir', type=str, default='./dataset')
-    parser.add_argument('--train_annotations_path', type=str, default='rf-detr/dataset/annotations/instances_train2017.json',
+    parser.add_argument('--train_annotations_path', type=str, default='./dataset/annotations/instances_train2017.json',
                         help='Path to the training annotations file.')
-    parser.add_argument('--val_annotations_path', type=str, default='rf-detr/dataset/annotations/instances_val2017.json',
+    parser.add_argument('--val_annotations_path', type=str, default='./dataset/annotations/instances_val2017.json',
                         help='Path to the validation annotations file.')
     parser.add_argument('--square_resize_div_64', action='store_true')
 

--- End Diff ---

84. 在文件 'rfdetr\engine.py' 中: 在 rfdetr/engine.py 文件中添加了对自定义类别的支持，新增了从 rfdetr.util.custom_classes 模块导入 CUSTOM_CLASSES 的语句。这一变更使得引擎能够处理自定义类别数据，扩展了原有的类别支持能力（之前仅支持 COCO_CLASSES）。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 069d1c7..f2b530b 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -42,6 +42,7 @@ from PIL import Image, ImageDraw, ImageFont
 import torchvision.transforms.functional as F
 from rfdetr.util import box_ops
 from rfdetr.util.coco_classes import COCO_CLASSES
+from rfdetr.util.custom_classes import CUSTOM_CLASSES
 import numpy as np
 
 def get_autocast_args(args):

--- End Diff ---

85. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

修改了`rfdetr/engine.py`中的类别名称获取逻辑，将默认使用的类别映射从固定的`COCO_CLASSES`更改为用户自定义的`CUSTOM_CLASSES`。具体变更包括：

1. 更新了注释，表明现在优先使用用户自定义类别映射而非COCO_CLASSES
2. 在获取类别映射的代码中，将回退方案从`COCO_CLASSES`改为`CUSTOM_CLASSES`
3. 在异常处理中也同样使用`CUSTOM_CLASSES`作为回退方案

这一变更增强了代码的灵活性，使其能够支持用户自定义的类别定义，而不再局限于COCO数据集的固定类别，提高了模型在不同数据集上的适用性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index f2b530b..416faa5 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -477,7 +477,7 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
             scaled_pred_boxes = box_ops.box_cxcywh_to_xyxy(pred_boxes)
             scaled_pred_boxes = scaled_pred_boxes * torch.tensor([img_w, img_h, img_w, img_h], device=scaled_pred_boxes.device)
             
-            # 改进的类别名称获取逻辑 - 使用COCO_CLASSES
+            # 改进的类别名称获取逻辑 - 优先使用用户自定义类别映射
             category_mapping = {}
             try:
                 # 从coco_dataset获取类别映射
@@ -486,14 +486,14 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                         category_mapping[cat_id] = cat_info['name']
                     print(f"从coco_dataset获取类别: {category_mapping}")
                 
-                # 如果coco_dataset没有类别信息，使用COCO_CLASSES
+                # 如果coco_dataset没有类别信息，使用用户自定义类别映射
                 if not category_mapping:
-                    category_mapping = COCO_CLASSES
-                    print(f"使用COCO_CLASSES映射: {len(category_mapping)}个类别")
+                    category_mapping = CUSTOM_CLASSES
+                    print(f"使用CUSTOM_CLASSES映射: {len(category_mapping)}个类别")
                 
             except Exception as e:
-                print(f"获取类别映射时出错: {e}, 使用COCO_CLASSES回退方案")
-                category_mapping = COCO_CLASSES
+                print(f"获取类别映射时出错: {e}, 使用CUSTOM_CLASSES回退方案")
+                category_mapping = CUSTOM_CLASSES
             
             # 过滤低置信度预测
             # 使用更合理的阈值来显示预测框

--- End Diff ---

86. 在文件 'rfdetr\config.py' 中: 变更摘要：
修改了rfdetr/config.py文件中ModelConfig类的group_detr参数默认值，从13调整为10。
--- Git Diff ---
diff --git a/rfdetr/config.py b/rfdetr/config.py
index f71cdc0..41db9d2 100644
--- a/rfdetr/config.py
+++ b/rfdetr/config.py
@@ -28,7 +28,7 @@ class ModelConfig(BaseModel):
     pretrain_weights: Optional[str] = None
     device: Literal["cpu", "cuda", "mps"] = DEVICE
     resolution: int = 560
-    group_detr: int = 13
+    group_detr: int = 10
     gradient_checkpointing: bool = False
 
 class RFDETRBaseConfig(ModelConfig):

--- End Diff ---

87. 在文件 'rfdetr\main.py' 中: 这个变更摘要如下：

在rfdetr/main.py文件中，将group_detr参数的默认值从13修改为10。这是一个参数调整的变更，可能影响模型的行为或性能，但未改变代码结构或逻辑。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 2cc77d7..d3a816e 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -615,7 +615,7 @@ def populate_args(
     sa_nheads=8,
     ca_nheads=8,
     num_queries=300,
-    group_detr=13,
+    group_detr=10,
     two_stage=False,
     projector_scale='P4',
     lite_refpoint_refine=False,

--- End Diff ---

88. 在文件 'rfdetr\main.py' 中: ## 变更摘要

修改了`rfdetr/main.py`中的`group_detr`参数默认值，从13调整为10。该参数用于控制DETR训练过程中的分组数量，目的是加速训练过程。这一调整可能会影响训练速度和模型性能。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index d3a816e..a395442 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -864,7 +864,7 @@ def get_args_parser():
                         help="Number of attention heads inside the transformer's cross-attentions")
     parser.add_argument('--num_queries', default=300, type=int,
                         help="Number of query slots")
-    parser.add_argument('--group_detr', default=13, type=int,
+    parser.add_argument('--group_detr', default=10, type=int,
                         help="Number of groups to speed up detr training")
     parser.add_argument('--two_stage', action='store_true')
     parser.add_argument('--projector_scale', default=['P4'], type=str, nargs='+', choices=('P3', 'P4', 'P5', 'P6'))

--- End Diff ---

89. 在文件 'rfdetr\engine.py' 中: # 变更摘要

将目标检测图像保存功能中的类别索引处理逻辑从COCO数据集固定类别映射改为支持用户自定义类别映射。主要修改包括：

1. 更新注释，将"COCO_CLASSES使用1-based索引(1-90)"改为"用户自定义类别映射使用1-based索引(1-10)"
2. 变量重命名：将`coco_category_id`改为`custom_category_id`，以反映新的使用场景
3. 修改相关打印信息，明确指出使用的是"用户自定义类别ID"而非"COCO类别ID"

此变更提高了代码的灵活性，使其不再局限于COCO数据集的90个固定类别，而是可以适应自定义的10个类别的场景。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 416faa5..e14cd0c 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -514,17 +514,17 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 # 获取类别名称 - 修复类别索引问题
                 # 模型输出的类别索引是从0开始的，其中0通常表示背景类别
-                # COCO_CLASSES使用的是1-based索引（1-90），没有0类别
-                # 因此我们需要将模型输出的索引+1来匹配COCO_CLASSES的索引
+                # 用户自定义类别映射使用的是1-based索引（1-10），没有0类别
+                # 因此我们需要将模型输出的索引+1来匹配用户自定义类别映射的索引
                 if category_id == 0:
                     continue  # 跳过背景类别
                 
-                # 将模型输出的类别索引转换为COCO类别索引（+1）
-                coco_category_id = category_id + 1
+                # 将模型输出的类别索引转换为用户自定义类别索引（+1）
+                custom_category_id = category_id + 1
                 
                 # 使用转换后的ID获取类别名称
-                category_name = category_mapping.get(coco_category_id, f"Class_{coco_category_id}")
-                print(f"使用COCO类别ID {coco_category_id} 映射到类别名称: {category_name}")
+                category_name = category_mapping.get(custom_category_id, f"Class_{custom_category_id}")
+                print(f"使用用户自定义类别ID {custom_category_id} 映射到类别名称: {category_name}")
                 
                 # 调试输出
                 print(f"类别ID: {category_id}, 映射名称: {category_name}, 置信度: {score_value:.3f}")

--- End Diff ---

90. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

增强了`save_detection_images`函数中类别映射获取过程的调试能力，主要变更包括：

1. **增加错误情况日志**：当coco_dataset为空或缺少cats属性时，添加相应的提示信息
2. **增强类别映射使用情况的日志输出**：
   - 使用CUSTOM_CLASSES时，打印映射内容详情
   - 使用coco_dataset类别映射时，打印类别数量
3. **完善异常处理日志**：在获取类别映射出错时，除了使用CUSTOM_CLASSES作为回退方案外，还打印其具体内容

这些变更提高了代码的可调试性，使开发者能够更清晰地了解类别映射的获取和使用过程，便于排查相关问题。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index e14cd0c..db92144 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -485,15 +485,21 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                     for cat_id, cat_info in coco_dataset.cats.items():
                         category_mapping[cat_id] = cat_info['name']
                     print(f"从coco_dataset获取类别: {category_mapping}")
+                else:
+                    print("coco_dataset为空或没有cats属性")
                 
                 # 如果coco_dataset没有类别信息，使用用户自定义类别映射
                 if not category_mapping:
                     category_mapping = CUSTOM_CLASSES
                     print(f"使用CUSTOM_CLASSES映射: {len(category_mapping)}个类别")
+                    print(f"CUSTOM_CLASSES内容: {CUSTOM_CLASSES}")
+                else:
+                    print(f"使用coco_dataset类别映射: {len(category_mapping)}个类别")
                 
             except Exception as e:
                 print(f"获取类别映射时出错: {e}, 使用CUSTOM_CLASSES回退方案")
                 category_mapping = CUSTOM_CLASSES
+                print(f"CUSTOM_CLASSES内容: {CUSTOM_CLASSES}")
             
             # 过滤低置信度预测
             # 使用更合理的阈值来显示预测框

--- End Diff ---

91. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

修改了`rfdetr/engine.py`文件中的`save_detection_images`函数，增强了调试输出信息：

1. 简化了调试信息文本，将"使用用户自定义类别ID"改为更简洁的"使用类别ID"
2. 新增了一行调试输出，打印完整的类别映射字典内容，便于调试类别ID到名称的映射关系

此变更有助于在目标检测图像保存过程中更好地追踪和调试类别映射问题。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index db92144..7040600 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -530,7 +530,8 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 # 使用转换后的ID获取类别名称
                 category_name = category_mapping.get(custom_category_id, f"Class_{custom_category_id}")
-                print(f"使用用户自定义类别ID {custom_category_id} 映射到类别名称: {category_name}")
+                print(f"使用类别ID {custom_category_id} 映射到类别名称: {category_name}")
+                print(f"类别映射内容: {category_mapping}")
                 
                 # 调试输出
                 print(f"类别ID: {category_id}, 映射名称: {category_name}, 置信度: {score_value:.3f}")

--- End Diff ---

92. 在文件 'rfdetr\engine.py' 中: 无法生成摘要，原始 diff 内容如下：
---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 7040600..934426a 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -527,6 +527,13 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 # 将模型输出的类别索引转换为用户自定义类别索引（+1）
                 custom_category_id = category_id + 1
+                print(f"模型输出的类别ID: {category_id}, 转换后的ID: {custom_category_id}")
+                
+                # 检查转换后的ID是否在用户自定义类别映射范围内
+                max_custom_id = max(CUSTOM_CLASSES.keys()) if CUSTOM_CLASSES else 0
+                print(f"用户自定义类别映射的最大ID: {max_custom_id}")
+                if custom_category_id > max_custom_id:
+                    print(f"警告: 类别ID {custom_category_id} 超出了用户自定义类别映射范围 [1, {max_custom_id}]")
                 
                 # 使用转换后的ID获取类别名称
                 category_name = category_mapping.get(custom_category_id, f"Class_{custom_category_id}")

---
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 7040600..934426a 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -527,6 +527,13 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 # 将模型输出的类别索引转换为用户自定义类别索引（+1）
                 custom_category_id = category_id + 1
+                print(f"模型输出的类别ID: {category_id}, 转换后的ID: {custom_category_id}")
+                
+                # 检查转换后的ID是否在用户自定义类别映射范围内
+                max_custom_id = max(CUSTOM_CLASSES.keys()) if CUSTOM_CLASSES else 0
+                print(f"用户自定义类别映射的最大ID: {max_custom_id}")
+                if custom_category_id > max_custom_id:
+                    print(f"警告: 类别ID {custom_category_id} 超出了用户自定义类别映射范围 [1, {max_custom_id}]")
                 
                 # 使用转换后的ID获取类别名称
                 category_name = category_mapping.get(custom_category_id, f"Class_{custom_category_id}")

--- End Diff ---

93. 在文件 'rfdetr\engine.py' 中: 变更摘要：修改了目标检测图像保存功能中的类别ID处理逻辑。变更将原来"模型输出类别索引+1转换为用户自定义类别索引"的方式改为"直接使用模型输出的类别ID作为用户自定义类别映射的ID"。此变更表明之前的假设（模型输出索引从0开始而用户自定义类别从1开始）可能不正确，模型输出的类别ID已经直接对应于用户自定义类别映射的ID。同时更新了相关注释和日志输出信息，以反映新的处理逻辑。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 934426a..bd64e2a 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -520,22 +520,22 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
                 
                 # 获取类别名称 - 修复类别索引问题
                 # 模型输出的类别索引是从0开始的，其中0通常表示背景类别
-                # 用户自定义类别映射使用的是1-based索引（1-10），没有0类别
-                # 因此我们需要将模型输出的索引+1来匹配用户自定义类别映射的索引
+                # 用户自定义类别映射使用的是1-based索引（1-10）
+                # 但模型输出的类别ID可能直接对应用户自定义类别映射的ID
                 if category_id == 0:
                     continue  # 跳过背景类别
                 
-                # 将模型输出的类别索引转换为用户自定义类别索引（+1）
-                custom_category_id = category_id + 1
-                print(f"模型输出的类别ID: {category_id}, 转换后的ID: {custom_category_id}")
+                # 直接使用模型输出的类别ID作为用户自定义类别映射的ID
+                custom_category_id = category_id
+                print(f"模型输出的类别ID: {category_id}, 直接使用的ID: {custom_category_id}")
                 
-                # 检查转换后的ID是否在用户自定义类别映射范围内
+                # 检查ID是否在用户自定义类别映射范围内
                 max_custom_id = max(CUSTOM_CLASSES.keys()) if CUSTOM_CLASSES else 0
                 print(f"用户自定义类别映射的最大ID: {max_custom_id}")
                 if custom_category_id > max_custom_id:
                     print(f"警告: 类别ID {custom_category_id} 超出了用户自定义类别映射范围 [1, {max_custom_id}]")
                 
-                # 使用转换后的ID获取类别名称
+                # 使用直接的ID获取类别名称
                 category_name = category_mapping.get(custom_category_id, f"Class_{custom_category_id}")
                 print(f"使用类别ID {custom_category_id} 映射到类别名称: {category_name}")
                 print(f"类别映射内容: {category_mapping}")

--- End Diff ---

94. 在文件 'rfdetr\main.py' 中: ## 变更摘要

修正了RF-DETR模型中检测头重新初始化的逻辑，当预训练模型类别数与用户配置不匹配时，现在使用用户配置的类别数（加1）而非预训练模型的类别数来重新初始化检测头，确保模型能正确适应目标数据集的类别数量。同时添加了中文注释说明此操作的意图。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index a395442..6c85e2d 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -102,7 +102,8 @@ class Model:
                     f"Pretrained model has {checkpoint_num_classes - 1} classes, but your model is configured for {args.num_classes} classes.\n"
                     f"The detection head of the model will be re-initialized to match your dataset."
                 )
-                self.reinitialize_detection_head(checkpoint_num_classes)
+                # 重新初始化检测头以匹配用户配置的类别数
+                self.reinitialize_detection_head(args.num_classes + 1)
             # add support to exclude_keys
             # e.g., when load object365 pretrain, do not load `class_embed.[weight, bias]`
             if args.pretrain_exclude_keys is not None:

--- End Diff ---

95. 在文件 'rfdetr\main.py' 中: ### 变更摘要

在`rfdetr/main.py`的`Model`类中添加了处理预训练模型权重加载时的检测头权重移除逻辑。具体变更：

1. 在重新初始化检测头后，新增代码从checkpoint中移除所有检测头相关的权重（以'class_embed'或'bbox_embed'开头的键）
2. 这样做是为了避免预训练模型的检测头权重与重新初始化的检测头之间出现形状不匹配的问题
3. 每次移除权重时会打印相应的日志信息

此变更解决了在加载预训练模型并重新初始化检测头时可能出现的权重形状不兼容问题，提高了模型加载的稳定性。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 6c85e2d..111b6ad 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -104,6 +104,12 @@ class Model:
                 )
                 # 重新初始化检测头以匹配用户配置的类别数
                 self.reinitialize_detection_head(args.num_classes + 1)
+                
+                # 从checkpoint中移除检测头相关的权重，以避免形状不匹配的问题
+                detection_head_keys = [key for key in checkpoint['model'].keys() if key.startswith('class_embed') or key.startswith('bbox_embed')]
+                for key in detection_head_keys:
+                    del checkpoint['model'][key]
+                    print(f"Removed {key} from checkpoint to avoid shape mismatch")
             # add support to exclude_keys
             # e.g., when load object365 pretrain, do not load `class_embed.[weight, bias]`
             if args.pretrain_exclude_keys is not None:

--- End Diff ---

96. 在文件 'rfdetr\engine.py' 中: 变更摘要：修改了检测图像保存逻辑，在文件名中增加了图像ID标识，以防止不同图像在保存时被覆盖。具体变更是在保存路径的文件名中添加了`_image_{image_id}`后缀，确保每个保存的图像都有唯一的文件名。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index bd64e2a..5caf723 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -633,8 +633,8 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
         print("Warning: `criterion` not provided to `save_detection_images`. Cannot determine which boxes are used for loss. Skipping image save.")
         return
 
-    # Save the modified image
-    save_path = os.path.join(img_output_dir, f"epoch_{epoch}.png")
+    # Save the modified image with unique filename to prevent overwriting
+    save_path = os.path.join(img_output_dir, f"epoch_{epoch}_image_{image_id}.png")
     img.save(save_path)
     print(f"Image saved to {save_path} with {drawn_boxes} labeled boxes")
     sys.stdout.flush()  # Ensure immediate output

--- End Diff ---

97. 在文件 'rfdetr\main.py' 中: 将 rfdetr/main.py 中的训练轮次（epochs）参数默认值从 12 增加到 100，以延长模型训练时间，可能旨在提高模型性能或适应更复杂的训练需求。
--- Git Diff ---
diff --git a/rfdetr/main.py b/rfdetr/main.py
index 111b6ad..25f568f 100644
--- a/rfdetr/main.py
+++ b/rfdetr/main.py
@@ -582,7 +582,7 @@ def populate_args(
     lr_encoder=1.5e-4,
     batch_size=2,
     weight_decay=1e-4,
-    epochs=12,
+    epochs=100,
     lr_drop=11,
     clip_max_norm=0.1,
     lr_vit_layer_decay=0.8,

--- End Diff ---

98. 在文件 'rfdetr\engine.py' 中: # 代码变更摘要

本次变更主要针对RFDetR训练过程中的可视化功能进行了优化，具体变更包括：

1. **可视化频率调整**：
   - 原代码：每5个epoch且每100个迭代步骤保存一次检测结果图像
   - 新代码：每个epoch只处理第一个batch的第一张图片进行可视化

2. **代表性图片选择机制**：
   - 新增`representative_image_ids`参数支持，允许指定特定的图片ID进行可视化
   - 如果未设置`representative_image_ids`，则默认保存每个epoch第一个batch的第一张图片
   - 如果设置了该参数，则只保存ID在指定列表中的图片

3. **保存文件命名简化**：
   - 原代码：文件名包含epoch和image_id（格式：`epoch_{epoch}_image_{image_id}.png`）
   - 新代码：文件名只包含epoch（格式：`epoch_{epoch}.png`），简化了文件命名

这些变更使训练过程的可视化更加精准和有针对性，减少了不必要的图像保存，同时通过代表性图片机制提高了可视化的有效性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 5caf723..c270d15 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -133,11 +133,17 @@ def train_one_epoch(
                 
                 # 新增的可视化步骤
                 # Save images periodically to check detection results
-                if epoch % 5 == 0 and data_iter_step % 100 == 0:
-                    try:
-                        save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
-                    except Exception as e:
-                        print(f"Error saving detection image in epoch {epoch}: {e}")
+                # 选择一些代表性的图片在每个epoch都保存其预测结果
+                if data_iter_step == 0:  # 只处理每个epoch的第一个batch的第一张图片
+                    image_id = new_targets[0]['image_id'].item()
+                    # 简化条件检查，确保每个epoch都能保存代表性图片
+                    # 如果没有设置representative_image_ids，则保存所有第一batch的第一张图片
+                    # 如果设置了representative_image_ids，则只保存其中的图片
+                    if representative_image_ids is None or image_id in representative_image_ids:
+                        try:
+                            save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
+                        except Exception as e:
+                            print(f"Error saving detection image in epoch {epoch}: {e}")
 
                 loss_dict = criterion(outputs, new_targets)
                 weight_dict = criterion.weight_dict
@@ -634,7 +640,7 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
         return
 
     # Save the modified image with unique filename to prevent overwriting
-    save_path = os.path.join(img_output_dir, f"epoch_{epoch}_image_{image_id}.png")
+    save_path = os.path.join(img_output_dir, f"epoch_{epoch}.png")
     img.save(save_path)
     print(f"Image saved to {save_path} with {drawn_boxes} labeled boxes")
     sys.stdout.flush()  # Ensure immediate output

--- End Diff ---

99. 在文件 'rfdetr\engine.py' 中: ### 变更摘要

在 `rfdetr/engine.py` 文件的 `train_one_epoch` 函数中，针对保存代表性图片预测结果的功能进行了以下改进：

1. **简化了注释**：将关于条件检查的注释简化为更清晰的描述，强调确保每个epoch都能保存代表性图片。

2. **增强了错误处理**：在保存检测图片的异常处理块中，添加了 `traceback` 模块的导入和 `traceback.print_exc()` 调用，以便在保存图片出错时打印详细的错误堆栈信息，有助于问题诊断和调试。

这些变更提高了代码的可读性和错误诊断能力，使得在训练过程中保存代表性图片时出现的问题更容易被定位和解决。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index c270d15..5510222 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -136,7 +136,7 @@ def train_one_epoch(
                 # 选择一些代表性的图片在每个epoch都保存其预测结果
                 if data_iter_step == 0:  # 只处理每个epoch的第一个batch的第一张图片
                     image_id = new_targets[0]['image_id'].item()
-                    # 简化条件检查，确保每个epoch都能保存代表性图片
+                    # 确保每个epoch都能保存代表性图片
                     # 如果没有设置representative_image_ids，则保存所有第一batch的第一张图片
                     # 如果设置了representative_image_ids，则只保存其中的图片
                     if representative_image_ids is None or image_id in representative_image_ids:
@@ -144,6 +144,8 @@ def train_one_epoch(
                             save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
                         except Exception as e:
                             print(f"Error saving detection image in epoch {epoch}: {e}")
+                            import traceback
+                            traceback.print_exc()  # 打印详细的错误信息
 
                 loss_dict = criterion(outputs, new_targets)
                 weight_dict = criterion.weight_dict

--- End Diff ---

100. 在文件 'rfdetr\engine.py' 中: 在 `rfdetr/engine.py` 文件的 `train_one_epoch` 函数中增加了调试日志功能，用于跟踪检测图像的保存过程。具体变更包括：

1. 在尝试保存检测图像前添加了日志输出，显示当前训练周期和图像ID
2. 在成功保存检测图像后添加了确认日志

这些变更有助于开发者更好地监控和诊断图像保存过程中可能出现的问题，提高了代码的可调试性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 5510222..fde60a4 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -140,8 +140,10 @@ def train_one_epoch(
                     # 如果没有设置representative_image_ids，则保存所有第一batch的第一张图片
                     # 如果设置了representative_image_ids，则只保存其中的图片
                     if representative_image_ids is None or image_id in representative_image_ids:
+                        print(f"Attempting to save detection image for epoch {epoch}, image_id: {image_id}")
                         try:
                             save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
+                            print(f"Successfully saved detection image for epoch {epoch}")
                         except Exception as e:
                             print(f"Error saving detection image in epoch {epoch}: {e}")
                             import traceback

--- End Diff ---

101. 在文件 'rfdetr\engine.py' 中: 本次变更主要对 `save_detection_images` 函数进行了增强，添加了调试信息和错误处理机制。具体变更包括：

1. 添加了详细的调试打印语句，包括 epoch 值、处理的 image_id、图像文件名和图像尺寸等信息，便于问题排查。

2. 增加了对 `coco_dataset` 的有效性检查，当其为 None 时会打印警告并提前返回，避免后续操作出现异常。

3. 使用 try-except 块包装图像信息获取代码，捕获可能的异常并打印错误信息，增强了函数的健壮性。

4. 添加了原始图像路径的打印输出，帮助定位图像文件查找问题。

这些变更提高了函数的可靠性和可调试性，便于在图像保存过程中定位和解决问题。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 5f5b251..416e676 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -406,14 +406,29 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     Saves images by drawing matched predicted boxes on top of the original images
     which already contain ground truth boxes.
     """
+    print(f"save_detection_images called with epoch: {epoch}")
+    
     # Get image_id and filename
     target = targets[0]
     image_id = target['image_id'].item()
-    img_info = coco_dataset.loadImgs(image_id)[0]
-    img_filename = img_info['file_name']
+    print(f"Processing image_id: {image_id}")
+    
+    # Check if coco_dataset is valid
+    if coco_dataset is None:
+        print("Warning: coco_dataset is None")
+        return
+    
+    try:
+        img_info = coco_dataset.loadImgs(image_id)[0]
+        img_filename = img_info['file_name']
+        print(f"Image filename: {img_filename}")
+    except Exception as e:
+        print(f"Error getting image info: {e}")
+        return
 
     # Load the original image from the visual dataset (which has GT boxes)
     original_image_path = os.path.join("dataset/visual/train", img_filename)
+    print(f"Looking for original image at: {original_image_path}")
     if not os.path.exists(original_image_path):
         # If not found, try to reconstruct from the tensor
         print(f"Warning: Original image not found at {original_image_path}. Reconstructing from tensor.")
@@ -463,6 +478,7 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     # Get original size for scaling boxes
     orig_size = target['orig_size']
     img_h, img_w = orig_size.cpu().numpy()
+    print(f"Image dimensions: {img_w}x{img_h}")
 
     # Prepare output directory
     img_name_without_ext = os.path.splitext(img_filename)[0]

--- End Diff ---

102. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

在 `rfdetr/engine.py` 文件的 `save_detection_images` 函数中增强了图像保存的错误处理机制。具体变更包括：

1. 添加了保存路径的预打印信息，显示尝试保存图像的位置
2. 将原有的图像保存操作包装在 try-except 块中，增加了异常捕获能力
3. 当图像保存失败时，现在会打印详细的错误信息和完整的堆栈跟踪，便于调试

此变更提高了代码的健壮性，使得在图像保存过程中出现问题时能够提供更详细的错误信息，有助于快速定位和解决问题。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 416e676..fcd8690 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -662,6 +662,12 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
 
     # Save the modified image with unique filename to prevent overwriting
     save_path = os.path.join(img_output_dir, f"epoch_{epoch}.png")
-    img.save(save_path)
-    print(f"Image saved to {save_path} with {drawn_boxes} labeled boxes")
+    print(f"Attempting to save image to: {save_path}")
+    try:
+        img.save(save_path)
+        print(f"Image saved to {save_path} with {drawn_boxes} labeled boxes")
+    except Exception as e:
+        print(f"Error saving image to {save_path}: {e}")
+        import traceback
+        traceback.print_exc()
     sys.stdout.flush()  # Ensure immediate output

--- End Diff ---

103. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

对 `rfdetr/engine.py` 中的 `save_detection_images` 函数进行了健壮性改进，主要增强了处理 coco_dataset 异常情况的能力。

### 核心变更：
1. **移除了提前返回机制**：当 coco_dataset 为 None 或加载图像信息失败时，不再直接终止函数执行。
2. **增加了回退处理机制**：在无法从 coco_dataset 获取图像文件名时，使用标准的COCO文件名格式（`{image_id:012d}.jpg`）作为回退方案。
3. **优化了错误处理流程**：将错误信息打印与回退处理结合，确保函数在任何情况下都能继续执行。

### 变更目的：
提高函数在面对数据集访问问题时的容错能力，确保图像保存功能即使在数据集信息不完整的情况下也能正常工作，增强了系统的稳定性和可靠性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index fcd8690..d19a26c 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -415,17 +415,19 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     
     # Check if coco_dataset is valid
     if coco_dataset is None:
-        print("Warning: coco_dataset is None")
-        return
+        print("Warning: coco_dataset is None, using fallback method")
+        # 使用一个简单的回退方法
+        img_filename = f"{image_id:012d}.jpg"  # COCO数据集的标准文件名格式
+    else:
+        try:
+            img_info = coco_dataset.loadImgs(image_id)[0]
+            img_filename = img_info['file_name']
+            print(f"Image filename: {img_filename}")
+        except Exception as e:
+            print(f"Error getting image info: {e}")
+            # 回退到标准的COCO文件名格式
+            img_filename = f"{image_id:012d}.jpg"
     
-    try:
-        img_info = coco_dataset.loadImgs(image_id)[0]
-        img_filename = img_info['file_name']
-        print(f"Image filename: {img_filename}")
-    except Exception as e:
-        print(f"Error getting image info: {e}")
-        return
-
     # Load the original image from the visual dataset (which has GT boxes)
     original_image_path = os.path.join("dataset/visual/train", img_filename)
     print(f"Looking for original image at: {original_image_path}")

--- End Diff ---

104. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

在 `rfdetr/engine.py` 文件的 `train_one_epoch` 函数中添加了一行调试打印语句，用于输出训练过程中的进度信息。具体变更是在训练循环开始处添加了打印当前周期(epoch)和数据迭代步骤(data_iter_step)的代码，这有助于开发者跟踪和监控模型训练的进度状态。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index d19a26c..1d4b415 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -99,6 +99,7 @@ def train_one_epoch(
     for data_iter_step, (samples, targets) in enumerate(
         metric_logger.log_every(data_loader, print_freq, header)
     ):
+        print(f"Processing epoch {epoch}, data_iter_step {data_iter_step}")
         it = start_steps + data_iter_step
         callback_dict = {
             "step": it,

--- End Diff ---

105. 在文件 'rfdetr\engine.py' 中: # 变更摘要：增强图像路径调试功能

在 `rfdetr/engine.py` 文件的 `save_detection_images` 函数中，添加了目录存在性检查和诊断信息输出功能。变更主要在尝试加载原始图像之前，增加了对 `dataset/visual/train` 目录的详细检查逻辑：

1. 检查视觉训练目录是否存在，不存在时输出警告并列出 dataset 目录内容
2. 目录存在时，输出确认信息并显示目录中的文件数量及前几个文件名
3. 这些诊断信息有助于在原始图像路径不存在时，快速定位问题原因

此变更提升了调试能力，便于开发者排查图像加载失败的问题。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 1d4b415..c259ebd 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -432,6 +432,26 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     # Load the original image from the visual dataset (which has GT boxes)
     original_image_path = os.path.join("dataset/visual/train", img_filename)
     print(f"Looking for original image at: {original_image_path}")
+    
+    # 检查目录是否存在
+    visual_train_dir = "dataset/visual/train"
+    if not os.path.exists(visual_train_dir):
+        print(f"Warning: Visual train directory does not exist: {visual_train_dir}")
+        # 列出dataset目录的内容
+        if os.path.exists("dataset"):
+            print("Contents of dataset directory:")
+            for item in os.listdir("dataset"):
+                print(f"  {item}")
+        else:
+            print("Dataset directory does not exist")
+    else:
+        print(f"Visual train directory exists: {visual_train_dir}")
+        # 列出目录中的文件数量
+        files = os.listdir(visual_train_dir)
+        print(f"Number of files in visual train directory: {len(files)}")
+        if len(files) > 0:
+            print(f"First few files: {files[:5]}")
+    
     if not os.path.exists(original_image_path):
         # If not found, try to reconstruct from the tensor
         print(f"Warning: Original image not found at {original_image_path}. Reconstructing from tensor.")

--- End Diff ---

106. 在文件 'rfdetr\engine.py' 中: ### 变更摘要

在`rfdetr/engine.py`文件的`save_detection_images`函数中添加了输出目录存在性检查和创建逻辑。具体变更包括：

1. 在准备输出目录的阶段，增加了检查`output_dir`是否存在的代码
2. 如果目录不存在，则自动创建该目录并打印确认信息
3. 这一修改增强了函数的健壮性，防止因输出目录不存在而导致的文件保存错误

此变更确保了在保存检测结果图像时，所需的目录结构会自动创建，提高了代码的可靠性和用户体验。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index c259ebd..6c9cc99 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -504,6 +504,11 @@ def save_detection_images(outputs, targets, samples, epoch, output_dir="./result
     print(f"Image dimensions: {img_w}x{img_h}")
 
     # Prepare output directory
+    # 确保输出目录存在
+    if not os.path.exists(output_dir):
+        os.makedirs(output_dir)
+        print(f"Created output directory: {output_dir}")
+    
     img_name_without_ext = os.path.splitext(img_filename)[0]
     img_output_dir = os.path.join(output_dir, img_name_without_ext)
     os.makedirs(img_output_dir, exist_ok=True)

--- End Diff ---

107. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

在`rfdetr/engine.py`文件中增加了调试日志输出，用于跟踪训练过程中的图像保存可视化功能。具体变更包括：

1. 在每个epoch开始时（第一个batch处理前）添加提示信息，表明将尝试保存可视化图像。

2. 在代表性图像保存逻辑部分增加详细调试输出：
   - 打印当前epoch和图像ID信息
   - 检查并显示`representative_image_ids`是否为None
   - 若`representative_image_ids`不为空，检查当前图像ID是否在代表性图像列表中
   - 显示代表性图像列表中的前几个ID（最多10个）

这些变更旨在提高代码的可调试性，帮助开发者更好地理解图像保存决策过程，便于排查相关问题。变更不改变原有功能逻辑，仅增加日志输出。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 6c9cc99..895319c 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -100,6 +100,10 @@ def train_one_epoch(
         metric_logger.log_every(data_loader, print_freq, header)
     ):
         print(f"Processing epoch {epoch}, data_iter_step {data_iter_step}")
+        # 在每个epoch开始时检查是否应该保存图像
+        if data_iter_step == 0:
+            print(f"First batch of epoch {epoch}, will attempt to save visualization")
+        
         it = start_steps + data_iter_step
         callback_dict = {
             "step": it,
@@ -137,6 +141,13 @@ def train_one_epoch(
                 # 选择一些代表性的图片在每个epoch都保存其预测结果
                 if data_iter_step == 0:  # 只处理每个epoch的第一个batch的第一张图片
                     image_id = new_targets[0]['image_id'].item()
+                    print(f"Checking image saving condition for epoch {epoch}, image_id: {image_id}")
+                    print(f"representative_image_ids is None: {representative_image_ids is None}")
+                    if representative_image_ids is not None:
+                        print(f"image_id {image_id} in representative_image_ids: {image_id in representative_image_ids}")
+                        if len(representative_image_ids) > 0:
+                            print(f"First few representative_image_ids: {list(representative_image_ids)[:10]}")
+                    
                     # 确保每个epoch都能保存代表性图片
                     # 如果没有设置representative_image_ids，则保存所有第一batch的第一张图片
                     # 如果设置了representative_image_ids，则只保存其中的图片

--- End Diff ---

108. 在文件 'rfdetr\engine.py' 中: ## 变更摘要

在`rfdetr/engine.py`文件的`train_one_epoch`函数中增加了调试信息输出功能。具体变更包括：

1. 添加了训练开始时的epoch信息打印
2. 保留了原有的数据加载器长度打印
3. 新增了对`representative_image_ids`参数的完整打印
4. 当`representative_image_ids`不为空时，额外打印前10个代表性图像ID

这些变更旨在增强训练过程的可观察性，便于调试和监控训练过程中的关键参数状态。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 895319c..c4a3490 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -95,7 +95,11 @@ def train_one_epoch(
     optimizer.zero_grad()
     assert batch_size % args.grad_accum_steps == 0
     sub_batch_size = batch_size // args.grad_accum_steps
-    print("LENGTH OF DATA LOADER:", len(data_loader))
+    print(f"Starting train_one_epoch for epoch {epoch}")
+    print(f"LENGTH OF DATA LOADER:", len(data_loader))
+    print(f"representative_image_ids: {representative_image_ids}")
+    if representative_image_ids is not None and len(representative_image_ids) > 0:
+        print(f"First few representative_image_ids: {list(representative_image_ids)[:10]}")
     for data_iter_step, (samples, targets) in enumerate(
         metric_logger.log_every(data_loader, print_freq, header)
     ):

--- End Diff ---

109. 在文件 'rfdetr\engine.py' 中: 本次变更修改了训练过程中保存代表性图片的逻辑。核心变化是从条件性保存（仅当图片ID在representative_image_ids中时才保存）改为无条件保存每个epoch的第一张图片。这一修改确保了无论是否设置了representative_image_ids，每个epoch都能看到检测结果的可视化输出，提高了训练过程的可观察性。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index c4a3490..9087c77 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -152,18 +152,18 @@ def train_one_epoch(
                         if len(representative_image_ids) > 0:
                             print(f"First few representative_image_ids: {list(representative_image_ids)[:10]}")
                     
-                    # 确保每个epoch都能保存代表性图片
+                    # 确保每个epoch都能保存第一张图片，而不依赖于representative_image_ids
                     # 如果没有设置representative_image_ids，则保存所有第一batch的第一张图片
                     # 如果设置了representative_image_ids，则只保存其中的图片
-                    if representative_image_ids is None or image_id in representative_image_ids:
-                        print(f"Attempting to save detection image for epoch {epoch}, image_id: {image_id}")
-                        try:
-                            save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
-                            print(f"Successfully saved detection image for epoch {epoch}")
-                        except Exception as e:
-                            print(f"Error saving detection image in epoch {epoch}: {e}")
-                            import traceback
-                            traceback.print_exc()  # 打印详细的错误信息
+                    # 但为了确保能看到结果，我们总是保存每个epoch的第一张图片
+                    print(f"Attempting to save detection image for epoch {epoch}, image_id: {image_id}")
+                    try:
+                        save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
+                        print(f"Successfully saved detection image for epoch {epoch}")
+                    except Exception as e:
+                        print(f"Error saving detection image in epoch {epoch}: {e}")
+                        import traceback
+                        traceback.print_exc()  # 打印详细的错误信息
 
                 loss_dict = criterion(outputs, new_targets)
                 weight_dict = criterion.weight_dict

--- End Diff ---

110. 在文件 'rfdetr\engine.py' 中: # 变更摘要：优化训练过程中的图像可视化保存逻辑

本次变更修改了`rfdetr/engine.py`文件中的`train_one_epoch`函数，主要优化了训练过程中保存检测图像的可视化逻辑：

## 核心变更
1. **保存策略调整**：从只保存每个epoch第一个batch的第一张图片，改为检查并保存代表性图片集合中的图片
2. **循环处理机制**：新增循环遍历当前batch中的所有图片，检查其image_id是否在representative_image_ids集合中
3. **单图像处理优化**：为每个待保存的图像创建单独的targets和samples对象，避免处理整个batch的数据

## 具体改动
- 移除了仅处理第一个batch第一张图片的限制条件
- 增加了对representative_image_ids集合的检查逻辑
- 优化了图像保存的数据处理方式，使用NestedTensor处理单张图像而非整个batch
- 增加了保存成功后的break语句，确保每个epoch只保存一张代表性图像

## 变更目的
提高训练过程中图像可视化的灵活性和针对性，使开发者能够更精确地监控特定图像在训练过程中的检测效果变化。
--- Git Diff ---
diff --git a/rfdetr/engine.py b/rfdetr/engine.py
index 9087c77..756f86e 100644
--- a/rfdetr/engine.py
+++ b/rfdetr/engine.py
@@ -143,8 +143,9 @@ def train_one_epoch(
                 # 新增的可视化步骤
                 # Save images periodically to check detection results
                 # 选择一些代表性的图片在每个epoch都保存其预测结果
-                if data_iter_step == 0:  # 只处理每个epoch的第一个batch的第一张图片
-                    image_id = new_targets[0]['image_id'].item()
+                # 检查当前batch中的图像是否在representative_image_ids中
+                for i, target in enumerate(new_targets):
+                    image_id = target['image_id'].item()
                     print(f"Checking image saving condition for epoch {epoch}, image_id: {image_id}")
                     print(f"representative_image_ids is None: {representative_image_ids is None}")
                     if representative_image_ids is not None:
@@ -152,18 +153,24 @@ def train_one_epoch(
                         if len(representative_image_ids) > 0:
                             print(f"First few representative_image_ids: {list(representative_image_ids)[:10]}")
                     
-                    # 确保每个epoch都能保存第一张图片，而不依赖于representative_image_ids
-                    # 如果没有设置representative_image_ids，则保存所有第一batch的第一张图片
-                    # 如果设置了representative_image_ids，则只保存其中的图片
-                    # 但为了确保能看到结果，我们总是保存每个epoch的第一张图片
-                    print(f"Attempting to save detection image for epoch {epoch}, image_id: {image_id}")
-                    try:
-                        save_detection_images(outputs, new_targets, new_samples, epoch, coco_dataset=coco_dataset, criterion=criterion)
-                        print(f"Successfully saved detection image for epoch {epoch}")
-                    except Exception as e:
-                        print(f"Error saving detection image in epoch {epoch}: {e}")
-                        import traceback
-                        traceback.print_exc()  # 打印详细的错误信息
+                    # 如果图像在representative_image_ids中，则保存它
+                    if representative_image_ids is None or image_id in representative_image_ids:
+                        print(f"Attempting to save detection image for epoch {epoch}, image_id: {image_id}")
+                        try:
+                            # 为当前目标创建新的targets和samples列表
+                            single_target = [target]
+                            single_sample_tensors = new_samples.tensors[i:i+1]
+                            single_sample_mask = new_samples.mask[i:i+1]
+                            single_sample = NestedTensor(single_sample_tensors, single_sample_mask)
+                            
+                            save_detection_images(outputs, single_target, single_sample, epoch, coco_dataset=coco_dataset, criterion=criterion)
+                            print(f"Successfully saved detection image for epoch {epoch}, image_id: {image_id}")
+                            # 一旦找到并保存了一个代表性图像，就跳出循环
+                            break
+                        except Exception as e:
+                            print(f"Error saving detection image in epoch {epoch}, image_id {image_id}: {e}")
+                            import traceback
+                            traceback.print_exc()  # 打印详细的错误信息
 
                 loss_dict = criterion(outputs, new_targets)
                 weight_dict = criterion.weight_dict

--- End Diff ---
